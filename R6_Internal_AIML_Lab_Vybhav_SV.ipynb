{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Questions - Internal - R6 AIML Labs .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zUZjPnVXGz0Z"
      },
      "source": [
        "# The Iris Dataset\n",
        "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
        "\n",
        "The dataset contains a set of 150 records under five attributes - petal length, petal width, sepal length, sepal width and species."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RMbmpriavLE9"
      },
      "source": [
        "### Specifying the TensorFlow version\n",
        "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fu8bUU__oa7h",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bLz1Ckvfvn6D"
      },
      "source": [
        "### Import TensorFlow\n",
        "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWrzVTLOvn6M",
        "outputId": "e9bcf2ab-f26a-429a-b97a-3e6477a85b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_uYeJgkNuXNC"
      },
      "source": [
        "### Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lcASNsewsfQX",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(47)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5-vVQBBqg7DI"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kE0EDKvQhEIe"
      },
      "source": [
        "### Import dataset\n",
        "- Import iris dataset\n",
        "- Import the dataset using sklearn library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IOOWpD26Haq3",
        "colab": {}
      },
      "source": [
        "# Import pandas \n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uqy0AcGb31y",
        "colab_type": "code",
        "outputId": "51fb6514-6c1a-4a71-fbcc-865c20bab492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bds4JgOleorf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris_data=pd.read_csv('gdrive/My Drive/Colab Notebooks/iris.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEzgL7sGfDA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "334718c5-9a8c-44c2-d833-61547a216690"
      },
      "source": [
        "iris_data.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>Species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
              "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
              "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
              "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
              "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
              "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ta8YqInTh5v5"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HERt3drbhX0i"
      },
      "source": [
        "### Get features and label from the dataset in separate variable\n",
        "- you can get the features using .data method\n",
        "- you can get the features using .target method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0cV-_qHAHyvE",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "\n",
        "X1 = iris_data.iloc[:,0:5]  #independent columns\n",
        "Y1 = iris_data.iloc[:,-1]   #target column\n",
        "\n",
        "X = data.data   #independent columns\n",
        "Y = data.target #target column"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znyRuv5-hYpL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1bdf274e-9a32-4eae-ca0f-82275b4e21f5"
      },
      "source": [
        "X"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WwlAu8FhiI3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "cdd6a45f-08f9-4089-f3f7-20fa5d6df1e7"
      },
      "source": [
        "Y"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qg1A2lkUjFak"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YErwYLCH0N_"
      },
      "source": [
        "### Create train and test data\n",
        "- use train_test_split to get train and test set\n",
        "- set a random_state\n",
        "- test_size: 0.25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TYKNJL85h7pQ",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, Y, test_size=0.25, random_state=9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js8DEakciFbf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0a92a422-708f-4ff2-eec8-b9d150201996"
      },
      "source": [
        "print(\"X_train shape -- > {}\".format(X_train.shape))\n",
        "print(\"y_train shape -- > {}\".format(Y_train.shape))\n",
        "print(\"X_test shape -- > {}\".format(X_test.shape))\n",
        "print(\"y_test shape -- > {}\".format(Y_test.shape))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape -- > (112, 4)\n",
            "y_train shape -- > (112,)\n",
            "X_test shape -- > (38, 4)\n",
            "y_test shape -- > (38,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g0KVP17Ozaix"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SIjqxbhWv1zv"
      },
      "source": [
        "### One-hot encode the labels\n",
        "- convert class vectors (integers) to binary class matrix\n",
        "- convert labels\n",
        "- number of classes: 3\n",
        "- we are doing this to use categorical_crossentropy as loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R9vv-_gpyLY9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "828c8293-7fd3-4da3-c801-fe4a738abc8a"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "OHE = OneHotEncoder(categories='auto')\n",
        "Y1_train = OHE.fit_transform(Y_train)\n",
        "Y1_test = OHE.fit_transform(Y_test)\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-159d7e67317e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mOHE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mY1_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOHE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mY1_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOHE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \"\"\"\n\u001b[1;32m    371\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \"\"\"\n\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_idx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_drop_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mX_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iloc'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# if not a dataframe, do normal check_array validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mX_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             if (not hasattr(X, 'dtype')\n\u001b[1;32m     45\u001b[0m                     and np.issubdtype(X_temp.dtype, np.str_)):\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[2 0 0 2 1 0 2 1 0 1 0 2 1 1 0 1 2 2 2 2 1 0 2 0 0 1 2 0 1 1 2 1 1 0 0 2 1\n 0 2 1 2 1 0 1 1 0 0 2 1 2 0 1 0 0 1 2 2 0 0 1 1 2 2 0 2 0 0 2 0 2 1 0 2 1\n 1 0 2 1 2 2 1 2 1 1 0 1 2 0 2 1 0 2 2 0 2 0 0 2 2 1 2 1 2 0 1 0 1 2 1 2 1\n 2].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nBmBjAGmItQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "Y_train = to_categorical(Y_train,3)\n",
        "Y_test = to_categorical(Y_test,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5zUpjCVmCU1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef81d51b-2bec-4a75-ba5c-ec2df6720581"
      },
      "source": [
        "Y_train"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2N7ByQPmPEx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "38e8f475-bd85-4611-8ad0-d217a70718ba"
      },
      "source": [
        "Y_test"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovjLyYzWkO9s"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hbIFzoPNSyYo"
      },
      "source": [
        "### Initialize a sequential model\n",
        "- Define a sequential model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4FvSbf1UjHtl",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "model = tf.keras.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dGMy999vlacX"
      },
      "source": [
        "## Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "72ibK5Jxm8iL"
      },
      "source": [
        "### Add a layer\n",
        "- Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3\n",
        "- Apply Softmax on Dense Layer outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZKrBNSRm_o9",
        "colab": {}
      },
      "source": [
        "model.add(layers.Dense(4, activation='relu', input_shape=(4,)))\n",
        "model.add(layers.Dense(3, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i4uiTH8plmNX"
      },
      "source": [
        "## Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yJL8n8vcSyYz"
      },
      "source": [
        "### Compile the model\n",
        "- Use SGD as Optimizer\n",
        "- Use categorical_crossentropy as loss function\n",
        "- Use accuracy as metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tc_-fjIEk1ve",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "886f18c1-428b-427a-f975-5177cb94b833"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, Y_train,epochs=20, batch_size=1, verbose=1)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 112 samples\n",
            "Epoch 1/20\n",
            "112/112 [==============================] - 1s 6ms/sample - loss: 1.2443 - accuracy: 0.3482\n",
            "Epoch 2/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 1.0605 - accuracy: 0.3571\n",
            "Epoch 3/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.9502 - accuracy: 0.5268\n",
            "Epoch 4/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.8293 - accuracy: 0.6429\n",
            "Epoch 5/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.7377 - accuracy: 0.6964\n",
            "Epoch 6/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.6848 - accuracy: 0.7321\n",
            "Epoch 7/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.6491 - accuracy: 0.6964\n",
            "Epoch 8/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.6084 - accuracy: 0.7411\n",
            "Epoch 9/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.5859 - accuracy: 0.7589\n",
            "Epoch 10/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.5652 - accuracy: 0.7500\n",
            "Epoch 11/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.5399 - accuracy: 0.7679\n",
            "Epoch 12/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.5164 - accuracy: 0.7500\n",
            "Epoch 13/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.5301 - accuracy: 0.7768\n",
            "Epoch 14/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.5043 - accuracy: 0.8036\n",
            "Epoch 15/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.4971 - accuracy: 0.8304\n",
            "Epoch 16/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.4783 - accuracy: 0.8214\n",
            "Epoch 17/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.4569 - accuracy: 0.7946\n",
            "Epoch 18/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.4346 - accuracy: 0.8750\n",
            "Epoch 19/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.4517 - accuracy: 0.8214\n",
            "Epoch 20/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.4346 - accuracy: 0.8839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sihIGbRll_jT"
      },
      "source": [
        "## Question 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "54ZZCfNGlu0i"
      },
      "source": [
        "### Summarize the model\n",
        "- Check model layers\n",
        "- Understand number of trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "elER3F_4ln8n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "3a32c067-0c5b-49fc-ce76-2c0e1a1026bf"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                multiple                  20        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  20        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  15        \n",
            "=================================================================\n",
            "Total params: 55\n",
            "Trainable params: 55\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2PiP7j3Vmj4p"
      },
      "source": [
        "## Question 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rWdbfFCXmCHt"
      },
      "source": [
        "### Fit the model\n",
        "- Give train data as training features and labels\n",
        "- Epochs: 100\n",
        "- Give validation data as testing features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cO1c-5tjmBVZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eddc9ebf-236b-4bf6-f207-38154f79fbe8"
      },
      "source": [
        "history1 = model.fit(X_train, Y_train, validation_data=(X_test,Y_test) ,epochs=100, batch_size=1, verbose=1)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 112 samples, validate on 38 samples\n",
            "Epoch 1/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.4331 - accuracy: 0.8571 - val_loss: 0.3948 - val_accuracy: 0.8684\n",
            "Epoch 2/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.4198 - accuracy: 0.8214 - val_loss: 0.4093 - val_accuracy: 0.9211\n",
            "Epoch 3/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.4152 - accuracy: 0.8482 - val_loss: 0.3929 - val_accuracy: 0.9474\n",
            "Epoch 4/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3960 - accuracy: 0.8929 - val_loss: 0.3923 - val_accuracy: 0.9211\n",
            "Epoch 5/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3778 - accuracy: 0.8661 - val_loss: 0.3519 - val_accuracy: 0.8947\n",
            "Epoch 6/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3931 - accuracy: 0.8839 - val_loss: 0.3438 - val_accuracy: 0.8947\n",
            "Epoch 7/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3820 - accuracy: 0.8929 - val_loss: 0.3391 - val_accuracy: 0.8947\n",
            "Epoch 8/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3542 - accuracy: 0.8839 - val_loss: 0.3593 - val_accuracy: 0.8158\n",
            "Epoch 9/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3476 - accuracy: 0.8929 - val_loss: 0.4368 - val_accuracy: 0.9211\n",
            "Epoch 10/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3576 - accuracy: 0.8929 - val_loss: 0.3122 - val_accuracy: 0.8947\n",
            "Epoch 11/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3331 - accuracy: 0.8839 - val_loss: 0.3453 - val_accuracy: 0.8158\n",
            "Epoch 12/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3214 - accuracy: 0.9107 - val_loss: 0.4852 - val_accuracy: 0.7632\n",
            "Epoch 13/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3437 - accuracy: 0.8571 - val_loss: 0.3198 - val_accuracy: 0.9474\n",
            "Epoch 14/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3121 - accuracy: 0.9018 - val_loss: 0.3310 - val_accuracy: 0.9474\n",
            "Epoch 15/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3328 - accuracy: 0.9286 - val_loss: 0.3539 - val_accuracy: 0.9211\n",
            "Epoch 16/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3223 - accuracy: 0.9018 - val_loss: 0.2831 - val_accuracy: 0.8947\n",
            "Epoch 17/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3091 - accuracy: 0.8839 - val_loss: 0.3006 - val_accuracy: 0.8684\n",
            "Epoch 18/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2902 - accuracy: 0.8929 - val_loss: 0.3043 - val_accuracy: 0.8421\n",
            "Epoch 19/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3051 - accuracy: 0.8839 - val_loss: 0.3189 - val_accuracy: 0.9474\n",
            "Epoch 20/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3285 - accuracy: 0.8929 - val_loss: 0.2838 - val_accuracy: 0.8947\n",
            "Epoch 21/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2971 - accuracy: 0.9196 - val_loss: 0.2913 - val_accuracy: 0.8684\n",
            "Epoch 22/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.3047 - accuracy: 0.9018 - val_loss: 0.3126 - val_accuracy: 0.9474\n",
            "Epoch 23/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2943 - accuracy: 0.8571 - val_loss: 0.2493 - val_accuracy: 0.8947\n",
            "Epoch 24/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2750 - accuracy: 0.9196 - val_loss: 0.2990 - val_accuracy: 0.9474\n",
            "Epoch 25/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2963 - accuracy: 0.9286 - val_loss: 0.2913 - val_accuracy: 0.9474\n",
            "Epoch 26/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2712 - accuracy: 0.9643 - val_loss: 0.2734 - val_accuracy: 0.8947\n",
            "Epoch 27/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2924 - accuracy: 0.9018 - val_loss: 0.2670 - val_accuracy: 0.9737\n",
            "Epoch 28/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2941 - accuracy: 0.9018 - val_loss: 0.2626 - val_accuracy: 0.8684\n",
            "Epoch 29/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2920 - accuracy: 0.9018 - val_loss: 0.2427 - val_accuracy: 0.9737\n",
            "Epoch 30/100\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.2584 - accuracy: 0.9375 - val_loss: 0.2601 - val_accuracy: 0.8947\n",
            "Epoch 31/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2667 - accuracy: 0.9286 - val_loss: 0.3178 - val_accuracy: 0.9211\n",
            "Epoch 32/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2659 - accuracy: 0.9196 - val_loss: 0.2530 - val_accuracy: 0.8684\n",
            "Epoch 33/100\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.2593 - accuracy: 0.9018 - val_loss: 0.2773 - val_accuracy: 0.9211\n",
            "Epoch 34/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2708 - accuracy: 0.9286 - val_loss: 0.2396 - val_accuracy: 0.9737\n",
            "Epoch 35/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2420 - accuracy: 0.9286 - val_loss: 0.2818 - val_accuracy: 0.9474\n",
            "Epoch 36/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2498 - accuracy: 0.9286 - val_loss: 0.2654 - val_accuracy: 0.9474\n",
            "Epoch 37/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2529 - accuracy: 0.9018 - val_loss: 0.2654 - val_accuracy: 0.9474\n",
            "Epoch 38/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2491 - accuracy: 0.9286 - val_loss: 0.2003 - val_accuracy: 0.9737\n",
            "Epoch 39/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2291 - accuracy: 0.9464 - val_loss: 0.3822 - val_accuracy: 0.8684\n",
            "Epoch 40/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2590 - accuracy: 0.8929 - val_loss: 0.2483 - val_accuracy: 0.9211\n",
            "Epoch 41/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2209 - accuracy: 0.9375 - val_loss: 0.2791 - val_accuracy: 0.9474\n",
            "Epoch 42/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2643 - accuracy: 0.9196 - val_loss: 0.3347 - val_accuracy: 0.8947\n",
            "Epoch 43/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2271 - accuracy: 0.9375 - val_loss: 0.2494 - val_accuracy: 0.9474\n",
            "Epoch 44/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2303 - accuracy: 0.9375 - val_loss: 0.3284 - val_accuracy: 0.9211\n",
            "Epoch 45/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2204 - accuracy: 0.9464 - val_loss: 0.2502 - val_accuracy: 0.9474\n",
            "Epoch 46/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2464 - accuracy: 0.9464 - val_loss: 0.2911 - val_accuracy: 0.9211\n",
            "Epoch 47/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2555 - accuracy: 0.9107 - val_loss: 0.4621 - val_accuracy: 0.7368\n",
            "Epoch 48/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2445 - accuracy: 0.9018 - val_loss: 0.1965 - val_accuracy: 0.9211\n",
            "Epoch 49/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2048 - accuracy: 0.9732 - val_loss: 0.2492 - val_accuracy: 0.8684\n",
            "Epoch 50/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2019 - accuracy: 0.9554 - val_loss: 0.1802 - val_accuracy: 0.9737\n",
            "Epoch 51/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2205 - accuracy: 0.9196 - val_loss: 0.1992 - val_accuracy: 0.8947\n",
            "Epoch 52/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2333 - accuracy: 0.9018 - val_loss: 0.2072 - val_accuracy: 0.9474\n",
            "Epoch 53/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2105 - accuracy: 0.9464 - val_loss: 0.2782 - val_accuracy: 0.9211\n",
            "Epoch 54/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2239 - accuracy: 0.9196 - val_loss: 0.1847 - val_accuracy: 0.9211\n",
            "Epoch 55/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2171 - accuracy: 0.9107 - val_loss: 0.1719 - val_accuracy: 0.9474\n",
            "Epoch 56/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1938 - accuracy: 0.9464 - val_loss: 0.2331 - val_accuracy: 0.9474\n",
            "Epoch 57/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2071 - accuracy: 0.9286 - val_loss: 0.2382 - val_accuracy: 0.9474\n",
            "Epoch 58/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2190 - accuracy: 0.9196 - val_loss: 0.3844 - val_accuracy: 0.7895\n",
            "Epoch 59/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2049 - accuracy: 0.9554 - val_loss: 0.1874 - val_accuracy: 0.9474\n",
            "Epoch 60/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1990 - accuracy: 0.9286 - val_loss: 0.3340 - val_accuracy: 0.8947\n",
            "Epoch 61/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2167 - accuracy: 0.9107 - val_loss: 0.2453 - val_accuracy: 0.9211\n",
            "Epoch 62/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1949 - accuracy: 0.9286 - val_loss: 0.2503 - val_accuracy: 0.9474\n",
            "Epoch 63/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2453 - accuracy: 0.8929 - val_loss: 0.1748 - val_accuracy: 0.9211\n",
            "Epoch 64/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1957 - accuracy: 0.9375 - val_loss: 0.2553 - val_accuracy: 0.9474\n",
            "Epoch 65/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2065 - accuracy: 0.9375 - val_loss: 0.1490 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1965 - accuracy: 0.9286 - val_loss: 0.1763 - val_accuracy: 0.9211\n",
            "Epoch 67/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1813 - accuracy: 0.9643 - val_loss: 0.3075 - val_accuracy: 0.9211\n",
            "Epoch 68/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2181 - accuracy: 0.9375 - val_loss: 0.1547 - val_accuracy: 0.9474\n",
            "Epoch 69/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2174 - accuracy: 0.8929 - val_loss: 0.2401 - val_accuracy: 0.8947\n",
            "Epoch 70/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1909 - accuracy: 0.9464 - val_loss: 0.4833 - val_accuracy: 0.7105\n",
            "Epoch 71/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2178 - accuracy: 0.9286 - val_loss: 0.1583 - val_accuracy: 0.9737\n",
            "Epoch 72/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1786 - accuracy: 0.9464 - val_loss: 0.2024 - val_accuracy: 0.8947\n",
            "Epoch 73/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2102 - accuracy: 0.9107 - val_loss: 0.1806 - val_accuracy: 0.9474\n",
            "Epoch 74/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1895 - accuracy: 0.9107 - val_loss: 0.1886 - val_accuracy: 0.9474\n",
            "Epoch 75/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1821 - accuracy: 0.9464 - val_loss: 0.2255 - val_accuracy: 0.9474\n",
            "Epoch 76/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2159 - accuracy: 0.9196 - val_loss: 0.1314 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1864 - accuracy: 0.9554 - val_loss: 0.1516 - val_accuracy: 0.9474\n",
            "Epoch 78/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1719 - accuracy: 0.9464 - val_loss: 0.1750 - val_accuracy: 0.9737\n",
            "Epoch 79/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1848 - accuracy: 0.9286 - val_loss: 0.1629 - val_accuracy: 0.9737\n",
            "Epoch 80/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1800 - accuracy: 0.9464 - val_loss: 0.1728 - val_accuracy: 0.9474\n",
            "Epoch 81/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1924 - accuracy: 0.9286 - val_loss: 0.3500 - val_accuracy: 0.8684\n",
            "Epoch 82/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2090 - accuracy: 0.9286 - val_loss: 0.1509 - val_accuracy: 0.9737\n",
            "Epoch 83/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1703 - accuracy: 0.9464 - val_loss: 0.4324 - val_accuracy: 0.7632\n",
            "Epoch 84/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2152 - accuracy: 0.9286 - val_loss: 0.1405 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.1636 - accuracy: 0.9554 - val_loss: 0.1591 - val_accuracy: 0.9737\n",
            "Epoch 86/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1971 - accuracy: 0.9464 - val_loss: 0.3750 - val_accuracy: 0.7895\n",
            "Epoch 87/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1836 - accuracy: 0.9286 - val_loss: 0.2247 - val_accuracy: 0.9211\n",
            "Epoch 88/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1702 - accuracy: 0.9554 - val_loss: 0.1664 - val_accuracy: 0.9474\n",
            "Epoch 89/100\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.1495 - accuracy: 0.9554 - val_loss: 0.2325 - val_accuracy: 0.9474\n",
            "Epoch 90/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1763 - accuracy: 0.9554 - val_loss: 0.2443 - val_accuracy: 0.9474\n",
            "Epoch 91/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.2038 - accuracy: 0.9286 - val_loss: 0.1306 - val_accuracy: 0.9737\n",
            "Epoch 92/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1826 - accuracy: 0.9464 - val_loss: 0.1165 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1643 - accuracy: 0.9464 - val_loss: 0.1861 - val_accuracy: 0.9474\n",
            "Epoch 94/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1776 - accuracy: 0.9196 - val_loss: 0.2493 - val_accuracy: 0.9211\n",
            "Epoch 95/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1756 - accuracy: 0.9554 - val_loss: 0.1727 - val_accuracy: 0.9474\n",
            "Epoch 96/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1615 - accuracy: 0.9643 - val_loss: 0.1567 - val_accuracy: 0.9474\n",
            "Epoch 97/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1566 - accuracy: 0.9643 - val_loss: 0.1526 - val_accuracy: 0.9737\n",
            "Epoch 98/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1563 - accuracy: 0.9464 - val_loss: 0.1851 - val_accuracy: 0.8947\n",
            "Epoch 99/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1539 - accuracy: 0.9554 - val_loss: 0.1313 - val_accuracy: 0.9737\n",
            "Epoch 100/100\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.1679 - accuracy: 0.9375 - val_loss: 0.1970 - val_accuracy: 0.9474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "re9ItAR3yS3J"
      },
      "source": [
        "## Question 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "liw0IFf9yVqH"
      },
      "source": [
        "### Make predictions\n",
        "- Predict labels on one row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H5sBybi6mlLl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "f8194e3c-d7e6-4ef9-a5f9-de6bb88092f9"
      },
      "source": [
        "y_pred = np.round(model.predict(X_test))\n",
        "y_pred"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo2Wq2u3v_35",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73e623f9-af1d-44de-dbdb-2bb95732dcc5"
      },
      "source": [
        "model.predict(X_test[ 0:1 , : ])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.5146032e-18, 2.3856441e-02, 9.7614360e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hSUgMq3m0bG7"
      },
      "source": [
        "### Compare the prediction with actual label\n",
        "- Print the same row as done in the previous step but of actual labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K5WbwVPyz-qQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aebfe2c0-c3c6-4b10-caa1-d59cb9fd2c0a"
      },
      "source": [
        "Y_test[0:1 , : ]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKU1eM-ozehw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "42da1ef1-6afc-4c40-bf4a-aa75e0a44fbd"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test,verbose=1)\n",
        "\n",
        "print(score)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "38/38 [==============================] - 0s 2ms/sample - loss: 0.1970 - accuracy: 0.9474\n",
            "[0.1970400747499968, 0.94736844]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FrTKwbgE7NFT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a1UBYPNp5Tn1"
      },
      "source": [
        "# Stock prices dataset\n",
        "The data is of tock exchange's stock listings for each trading day of 2010 to 2016.\n",
        "\n",
        "## Description\n",
        "A brief description of columns.\n",
        "- open: The opening market price of the equity symbol on the date\n",
        "- high: The highest market price of the equity symbol on the date\n",
        "- low: The lowest recorded market price of the equity symbol on the date\n",
        "- close: The closing recorded price of the equity symbol on the date\n",
        "- symbol: Symbol of the listed company\n",
        "- volume: Total traded volume of the equity symbol on the date\n",
        "- date: Date of record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ctH_ZW5g-M3g"
      },
      "source": [
        "### Specifying the TensorFlow version\n",
        "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vQbdODpH-M3r",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nFQWH1tj-M38"
      },
      "source": [
        "### Import TensorFlow\n",
        "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ho5n-xhd-M3_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0784ab13-039c-440a-e6b9-839fbf549b13"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tgkl0qu6-M4F"
      },
      "source": [
        "### Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TKgTyuA3-M4G",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(47)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_88voqAH-O6J"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dRHCeJqP-evf"
      },
      "source": [
        "### Load the data\n",
        "- load the csv file and read it using pandas\n",
        "- file name is prices.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cKVH5v7r-RmC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "44252785-3c01-42bf-d37e-18b10d8ca320"
      },
      "source": [
        "# run this cell to upload file if you are using google colab\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f55c2f4-a28f-4002-a251-2d89cea9713e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-9f55c2f4-a28f-4002-a251-2d89cea9713e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-5c2e8a8d365b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Cannot read property '_uploadFiles' of undefined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gDC6cSW_FSK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1f400043-2f53-431e-81cd-36b21344882b"
      },
      "source": [
        "prices=pd.read_csv('gdrive/My Drive/Colab Notebooks/prices.csv')\n",
        "prices.head()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>symbol</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-01-05 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-01-06 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-01-07 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-01-08 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-01-11 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  date symbol        open  ...         low        high     volume\n",
              "0  2016-01-05 00:00:00   WLTW  123.430000  ...  122.309998  126.250000  2163600.0\n",
              "1  2016-01-06 00:00:00   WLTW  125.239998  ...  119.940002  125.540001  2386400.0\n",
              "2  2016-01-07 00:00:00   WLTW  116.379997  ...  114.930000  119.739998  2489500.0\n",
              "3  2016-01-08 00:00:00   WLTW  115.480003  ...  113.500000  117.440002  2006300.0\n",
              "4  2016-01-11 00:00:00   WLTW  117.010002  ...  114.089996  117.330002  1408600.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HlLKVPVH_BCT"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9J4BlzVA_gZd"
      },
      "source": [
        "### Drop columnns\n",
        "- drop \"date\" and \"symbol\" column from the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IKEK8aEE_Csx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fc6671e9-6b48-437a-904d-ad78de7b2ff6"
      },
      "source": [
        "df = prices.drop(['date', 'symbol'], axis = 1) \n",
        "df.head()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high     volume\n",
              "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
              "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
              "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
              "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
              "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cTPhO6v-AiZt"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SsZXmF3NAkna"
      },
      "source": [
        "### Take initial rows\n",
        "- Take first 1000 rows from the data\n",
        "- This step is done to make the execution faster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aKs04iIHAjxN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd861a31-80d1-4558-9fc1-da9eefde2ed5"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(851264, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrS5fBIw1KMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "4174e88f-3551-46ac-ec6e-37ba63219e7c"
      },
      "source": [
        "df.iloc[0:1000 , :]"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>63.310001</td>\n",
              "      <td>63.590000</td>\n",
              "      <td>63.240002</td>\n",
              "      <td>63.639999</td>\n",
              "      <td>2133200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>27.160000</td>\n",
              "      <td>26.990000</td>\n",
              "      <td>26.680000</td>\n",
              "      <td>27.299999</td>\n",
              "      <td>1982400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>28.320000</td>\n",
              "      <td>28.770000</td>\n",
              "      <td>28.010000</td>\n",
              "      <td>28.809999</td>\n",
              "      <td>37152800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>44.000000</td>\n",
              "      <td>44.799999</td>\n",
              "      <td>43.750000</td>\n",
              "      <td>44.810001</td>\n",
              "      <td>6568600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>36.080002</td>\n",
              "      <td>37.139999</td>\n",
              "      <td>36.009998</td>\n",
              "      <td>37.230000</td>\n",
              "      <td>5604300.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           open       close         low        high      volume\n",
              "0    123.430000  125.839996  122.309998  126.250000   2163600.0\n",
              "1    125.239998  119.980003  119.940002  125.540001   2386400.0\n",
              "2    116.379997  114.949997  114.930000  119.739998   2489500.0\n",
              "3    115.480003  116.620003  113.500000  117.440002   2006300.0\n",
              "4    117.010002  114.970001  114.089996  117.330002   1408600.0\n",
              "..          ...         ...         ...         ...         ...\n",
              "995   63.310001   63.590000   63.240002   63.639999   2133200.0\n",
              "996   27.160000   26.990000   26.680000   27.299999   1982400.0\n",
              "997   28.320000   28.770000   28.010000   28.809999  37152800.0\n",
              "998   44.000000   44.799999   43.750000   44.810001   6568600.0\n",
              "999   36.080002   37.139999   36.009998   37.230000   5604300.0\n",
              "\n",
              "[1000 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l7yogRh1yPZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e35ce3a-7016-4392-baa8-bc5cf386e053"
      },
      "source": [
        "prices_df = df.iloc[0:1000 , :]\n",
        "prices_df.shape"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6vGtnapgBIJm"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C8u_jlbABTip"
      },
      "source": [
        "### Get features and label from the dataset in separate variable\n",
        "- Take \"open\", \"close\", \"low\", \"high\" columns as features\n",
        "- Take \"volume\" column as label\n",
        "- Normalize label column by dividing it with 1000000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xQjCMzUXBJbg",
        "colab": {}
      },
      "source": [
        "labels = prices_df['volume']\n",
        "features = prices_df.drop('volume', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLx90lHP3S2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = labels / 1000000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lujhavq52ojq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e4a886b0-7fa1-481d-e5dc-2280a9f744a2"
      },
      "source": [
        "labels"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       2.1636\n",
              "1       2.3864\n",
              "2       2.4895\n",
              "3       2.0063\n",
              "4       1.4086\n",
              "        ...   \n",
              "995     2.1332\n",
              "996     1.9824\n",
              "997    37.1528\n",
              "998     6.5686\n",
              "999     5.6043\n",
              "Name: volume, Length: 1000, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZQ96fDw2pnt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "5e8974b9-dbcf-4da9-9f39-fafcb05157e2"
      },
      "source": [
        "features"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>63.310001</td>\n",
              "      <td>63.590000</td>\n",
              "      <td>63.240002</td>\n",
              "      <td>63.639999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>27.160000</td>\n",
              "      <td>26.990000</td>\n",
              "      <td>26.680000</td>\n",
              "      <td>27.299999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>28.320000</td>\n",
              "      <td>28.770000</td>\n",
              "      <td>28.010000</td>\n",
              "      <td>28.809999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>44.000000</td>\n",
              "      <td>44.799999</td>\n",
              "      <td>43.750000</td>\n",
              "      <td>44.810001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>36.080002</td>\n",
              "      <td>37.139999</td>\n",
              "      <td>36.009998</td>\n",
              "      <td>37.230000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           open       close         low        high\n",
              "0    123.430000  125.839996  122.309998  126.250000\n",
              "1    125.239998  119.980003  119.940002  125.540001\n",
              "2    116.379997  114.949997  114.930000  119.739998\n",
              "3    115.480003  116.620003  113.500000  117.440002\n",
              "4    117.010002  114.970001  114.089996  117.330002\n",
              "..          ...         ...         ...         ...\n",
              "995   63.310001   63.590000   63.240002   63.639999\n",
              "996   27.160000   26.990000   26.680000   27.299999\n",
              "997   28.320000   28.770000   28.010000   28.809999\n",
              "998   44.000000   44.799999   43.750000   44.810001\n",
              "999   36.080002   37.139999   36.009998   37.230000\n",
              "\n",
              "[1000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aTAKzlxZBz0z"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IfY8Km1Zzyt2"
      },
      "source": [
        "### Convert data\n",
        "- Convert features and labels to numpy array\n",
        "- Convert their data type to \"float32\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ko7nnQVbYENh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e471e9e2-3191-4008-873f-7cea715609b2"
      },
      "source": [
        "X = np.array(features).astype('float32')\n",
        "Y= np.array(labels).astype('float32')\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 4)\n",
            "(1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bzE5lXn4Xpi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5d8da578-902a-47e1-e92a-cb502bfdcd27"
      },
      "source": [
        "X"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[123.43, 125.84, 122.31, 126.25],\n",
              "       [125.24, 119.98, 119.94, 125.54],\n",
              "       [116.38, 114.95, 114.93, 119.74],\n",
              "       ...,\n",
              "       [ 28.32,  28.77,  28.01,  28.81],\n",
              "       [ 44.  ,  44.8 ,  43.75,  44.81],\n",
              "       [ 36.08,  37.14,  36.01,  37.23]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlxEyxbk4YsL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "400ee6a1-a536-4a86-f0de-22af159d09ca"
      },
      "source": [
        "Y"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.163600e+00, 2.386400e+00, 2.489500e+00, 2.006300e+00,\n",
              "       1.408600e+00, 1.098000e+00, 9.496000e-01, 7.853000e-01,\n",
              "       1.093700e+00, 1.523500e+00, 1.653900e+00, 9.443000e-01,\n",
              "       7.449000e-01, 7.038000e-01, 5.631000e-01, 8.961000e-01,\n",
              "       6.804000e-01, 7.499000e-01, 5.742000e-01, 6.948000e-01,\n",
              "       8.963000e-01, 9.563000e-01, 9.971000e-01, 1.200500e+00,\n",
              "       1.725200e+00, 1.946000e+00, 1.319500e+00, 9.224000e-01,\n",
              "       1.185100e+00, 9.215000e-01, 4.409000e-01, 1.244300e+00,\n",
              "       6.813000e-01, 4.112000e-01, 4.473000e-01, 5.592000e-01,\n",
              "       4.599000e-01, 9.716000e-01, 6.941000e-01, 1.159600e+00,\n",
              "       8.847000e-01, 6.920000e-01, 8.980000e-01, 8.445000e-01,\n",
              "       7.965000e-01, 4.168000e-01, 5.902000e-01, 4.951000e-01,\n",
              "       5.158000e-01, 5.621000e-01, 8.093000e-01, 1.607300e+00,\n",
              "       1.001300e+00, 7.114000e-01, 4.442000e-01, 5.341000e-01,\n",
              "       7.982000e-01, 5.910000e-01, 4.432000e-01, 5.780000e-01,\n",
              "       7.651000e-01, 5.143000e-01, 1.150000e+00, 2.035300e+00,\n",
              "       7.022000e-01, 6.891000e-01, 5.532000e-01, 7.759000e-01,\n",
              "       6.217000e-01, 3.581000e-01, 9.393000e-01, 7.355000e-01,\n",
              "       9.246000e-01, 6.588000e-01, 1.055400e+00, 6.953000e-01,\n",
              "       4.600000e-01, 5.329000e-01, 4.626000e-01, 9.107000e-01,\n",
              "       8.896000e-01, 7.347000e-01, 1.304800e+00, 1.597300e+00,\n",
              "       1.917500e+00, 3.221500e+00, 9.198000e-01, 1.991600e+00,\n",
              "       5.948000e-01, 4.758000e-01, 7.151000e-01, 5.285000e-01,\n",
              "       5.800000e-01, 6.297000e-01, 5.982000e-01, 8.167000e-01,\n",
              "       6.408000e-01, 7.611000e-01, 4.621000e-01, 6.270000e-01,\n",
              "       7.225000e-01, 1.384200e+00, 4.415000e-01, 4.236000e-01,\n",
              "       4.967000e-01, 5.367000e-01, 4.294000e-01, 7.191000e-01,\n",
              "       3.945000e-01, 5.480000e-01, 2.633000e-01, 3.293000e-01,\n",
              "       3.787000e-01, 4.048000e-01, 6.593000e-01, 5.503000e-01,\n",
              "       4.856000e-01, 5.619000e-01, 5.123000e-01, 1.638000e+00,\n",
              "       8.844000e-01, 7.952000e-01, 1.049700e+00, 1.162100e+00,\n",
              "       8.584000e-01, 7.614000e-01, 7.499000e-01, 5.911000e-01,\n",
              "       6.928000e-01, 4.826000e-01, 8.440000e-01, 6.300000e-01,\n",
              "       5.671000e-01, 5.122000e-01, 4.622000e-01, 7.061000e-01,\n",
              "       5.339000e-01, 3.747000e-01, 3.738000e-01, 3.657000e-01,\n",
              "       3.139000e-01, 4.010000e-01, 2.755000e-01, 4.267000e-01,\n",
              "       5.746000e-01, 7.932000e-01, 4.469000e-01, 7.667000e-01,\n",
              "       1.865200e+00, 9.308000e-01, 5.027000e-01, 7.097000e-01,\n",
              "       5.639000e-01, 3.429000e-01, 4.788000e-01, 5.018000e-01,\n",
              "       4.103000e-01, 4.438000e-01, 1.414800e+00, 5.707000e-01,\n",
              "       3.913000e-01, 4.252000e-01, 5.263000e-01, 5.147000e-01,\n",
              "       4.565000e-01, 4.141000e-01, 4.645000e-01, 5.262000e-01,\n",
              "       4.873000e-01, 7.523000e-01, 6.325000e-01, 7.283000e-01,\n",
              "       6.411000e-01, 6.555000e-01, 4.889000e-01, 7.419000e-01,\n",
              "       7.134000e-01, 1.847200e+00, 3.107000e-01, 3.110000e-01,\n",
              "       9.277000e-01, 7.165000e-01, 7.970000e-01, 4.103000e-01,\n",
              "       5.036000e-01, 5.672000e-01, 1.504500e+00, 1.461900e+00,\n",
              "       1.142100e+00, 1.068500e+00, 7.002000e-01, 7.672000e-01,\n",
              "       8.551000e-01, 4.238000e-01, 5.757000e-01, 6.819000e-01,\n",
              "       5.403000e-01, 5.480000e-01, 5.645000e-01, 5.056000e-01,\n",
              "       4.356000e-01, 5.548000e-01, 4.966000e-01, 3.120000e-01,\n",
              "       4.147000e-01, 4.783000e-01, 4.170000e-01, 4.457000e-01,\n",
              "       4.592000e-01, 4.758000e-01, 6.932000e-01, 7.367000e-01,\n",
              "       3.531200e+00, 1.345600e+00, 9.676000e-01, 1.291600e+00,\n",
              "       1.524500e+00, 7.543000e-01, 1.089100e+00, 6.397000e-01,\n",
              "       8.184000e-01, 5.245000e-01, 8.190000e-01, 4.759000e-01,\n",
              "       1.011700e+00, 3.802000e-01, 2.840000e-01, 4.399000e-01,\n",
              "       3.529000e-01, 6.767000e-01, 8.987000e-01, 8.558000e-01,\n",
              "       6.047000e-01, 1.301200e+00, 9.283000e-01, 9.885000e-01,\n",
              "       9.434000e-01, 8.225000e-01, 1.168200e+00, 8.464000e-01,\n",
              "       8.266000e-01, 1.232700e+00, 9.376000e-01, 6.515000e-01,\n",
              "       6.146000e-01, 5.572000e-01, 3.619000e-01, 3.829000e-01,\n",
              "       4.299000e-01, 2.166000e-01, 4.664000e-01, 3.815500e+00,\n",
              "       9.837300e+00, 1.701700e+00, 1.234324e+02, 2.455900e+00,\n",
              "       1.082900e+01, 3.650100e+00, 4.710200e+00, 2.102700e+00,\n",
              "       3.472500e+00, 3.930100e+00, 7.943000e-01, 2.228600e+00,\n",
              "       1.299300e+00, 4.076600e+00, 4.597600e+00, 5.671300e+00,\n",
              "       2.362000e+00, 8.564000e-01, 7.750900e+00, 1.228200e+00,\n",
              "       3.793000e-01, 3.015600e+00, 7.129000e-01, 1.802400e+00,\n",
              "       2.631000e+00, 1.128200e+00, 1.861510e+01, 8.767000e-01,\n",
              "       3.061000e-01, 5.277400e+00, 2.238700e+00, 2.750500e+00,\n",
              "       7.599900e+00, 1.650400e+00, 4.641800e+00, 3.407100e+00,\n",
              "       2.364900e+00, 3.324500e+00, 1.131400e+00, 2.457200e+00,\n",
              "       1.151210e+01, 9.306600e+00, 1.483400e+00, 5.387000e-01,\n",
              "       1.301200e+00, 2.176100e+00, 6.894300e+00, 8.292000e-01,\n",
              "       4.083000e-01, 6.186700e+00, 1.808452e+02, 1.146750e+01,\n",
              "       2.971500e+00, 4.550800e+00, 6.433800e+00, 1.371800e+00,\n",
              "       1.793200e+00, 3.746700e+00, 5.888000e+00, 2.469700e+00,\n",
              "       6.127700e+00, 2.387000e-01, 1.234200e+00, 1.437610e+01,\n",
              "       1.433230e+01, 1.286000e+00, 1.511500e+00, 4.067930e+01,\n",
              "       3.385100e+00, 4.009700e+00, 3.824400e+00, 7.325600e+00,\n",
              "       2.670300e+00, 4.553000e+00, 6.710900e+00, 1.964100e+00,\n",
              "       5.372700e+00, 4.832000e+00, 3.055200e+00, 6.172000e+00,\n",
              "       2.232400e+00, 3.114680e+01, 8.229000e-01, 3.227300e+00,\n",
              "       1.261400e+00, 4.874200e+00, 1.014300e+00, 1.438400e+00,\n",
              "       1.357340e+01, 2.660500e+00, 3.139000e-01, 1.148600e+00,\n",
              "       5.875500e+00, 6.600000e-01, 2.239700e+00, 4.447400e+00,\n",
              "       4.514800e+00, 3.350600e+00, 8.281000e-01, 4.812000e-01,\n",
              "       1.388080e+01, 3.280200e+00, 1.574200e+00, 7.906000e+00,\n",
              "       5.985370e+01, 8.391000e+00, 1.051100e+00, 2.068100e+00,\n",
              "       5.765200e+00, 1.680700e+00, 1.467680e+01, 1.017380e+01,\n",
              "       3.025000e-01, 2.175500e+00, 1.448250e+01, 6.017600e+00,\n",
              "       3.974600e+00, 7.552600e+00, 4.372000e-01, 1.046000e+00,\n",
              "       5.799100e+00, 5.202600e+00, 1.370040e+01, 2.844100e+00,\n",
              "       5.354000e-01, 1.306900e+00, 3.059400e+00, 4.995000e-01,\n",
              "       1.502200e+00, 1.693300e+01, 1.199700e+00, 2.228100e+00,\n",
              "       1.421600e+00, 4.152100e+00, 1.910400e+00, 4.225500e+00,\n",
              "       3.850500e+00, 2.251160e+01, 1.009500e+00, 2.142300e+00,\n",
              "       7.507000e-01, 1.931500e+00, 3.508400e+00, 1.446400e+00,\n",
              "       3.781000e+00, 2.401200e+00, 4.110000e+00, 5.763000e-01,\n",
              "       3.299500e+00, 7.742000e-01, 1.206800e+00, 3.687800e+00,\n",
              "       4.480000e-01, 5.231900e+00, 2.347600e+00, 1.049400e+00,\n",
              "       1.030800e+00, 4.367600e+00, 1.062400e+00, 2.186500e+00,\n",
              "       7.744000e-01, 6.085580e+01, 3.432000e+00, 1.808240e+01,\n",
              "       3.215100e+00, 1.840900e+00, 6.749000e-01, 4.623700e+00,\n",
              "       2.536000e+00, 1.542910e+01, 1.873800e+00, 2.269800e+00,\n",
              "       3.553300e+00, 2.161800e+00, 1.789400e+00, 2.849400e+00,\n",
              "       1.651270e+01, 6.624000e-01, 1.636000e+00, 1.908400e+00,\n",
              "       4.030400e+00, 1.198500e+00, 6.707990e+01, 2.997000e+00,\n",
              "       1.680900e+01, 5.156400e+00, 1.653910e+01, 3.927000e+00,\n",
              "       3.908400e+00, 8.369000e-01, 2.533400e+00, 9.185600e+00,\n",
              "       2.098800e+00, 9.135000e+00, 3.905600e+00, 5.352000e-01,\n",
              "       1.157160e+01, 6.076000e-01, 1.652600e+00, 1.046280e+01,\n",
              "       3.658400e+00, 1.426600e+00, 4.278700e+00, 1.312090e+01,\n",
              "       3.729300e+00, 6.944100e+00, 2.904000e+00, 4.011600e+00,\n",
              "       7.389200e+00, 9.200000e-01, 2.795740e+01, 3.018000e+00,\n",
              "       2.705200e+00, 6.546000e-01, 7.948000e-01, 6.997800e+00,\n",
              "       1.048000e+00, 2.008500e+00, 6.155300e+00, 3.637000e+00,\n",
              "       3.252000e-01, 2.860000e-01, 1.793700e+00, 4.780090e+01,\n",
              "       2.353000e+00, 4.034900e+00, 4.444300e+00, 2.965200e+00,\n",
              "       9.955000e-01, 3.371000e-01, 2.728100e+00, 4.254200e+00,\n",
              "       1.876000e+00, 8.727000e+00, 1.728500e+00, 9.506200e+00,\n",
              "       3.332500e+00, 3.546050e+01, 2.816600e+00, 2.749000e+00,\n",
              "       1.490160e+01, 6.263800e+00, 2.827000e+00, 1.634900e+00,\n",
              "       2.122800e+00, 1.387040e+01, 1.271880e+01, 3.239900e+00,\n",
              "       6.135000e-01, 2.210900e+00, 3.959400e+00, 1.241400e+00,\n",
              "       4.493800e+00, 8.834000e-01, 1.656200e+00, 8.989000e-01,\n",
              "       2.887600e+00, 6.067100e+00, 2.408300e+00, 3.811400e+00,\n",
              "       1.332800e+00, 9.616700e+00, 1.856900e+00, 9.378000e-01,\n",
              "       9.625000e+00, 1.141850e+01, 8.823100e+00, 9.321000e+00,\n",
              "       3.858000e-01, 2.642400e+00, 2.860300e+00, 3.380500e+00,\n",
              "       2.975900e+00, 5.839300e+00, 1.518700e+00, 2.035200e+00,\n",
              "       1.859700e+00, 7.953300e+00, 6.109400e+00, 4.767000e+00,\n",
              "       4.970000e-01, 6.010500e+00, 4.900000e-01, 4.562000e-01,\n",
              "       3.680300e+00, 3.043700e+00, 3.880200e+00, 1.104860e+01,\n",
              "       3.424300e+00, 4.072100e+00, 1.389650e+01, 9.214200e+00,\n",
              "       3.840910e+01, 7.511700e+00, 7.210000e-01, 9.510000e-02,\n",
              "       3.441270e+01, 1.642300e+00, 3.611900e+00, 2.227600e+00,\n",
              "       2.171500e+00, 2.602200e+00, 5.625400e+00, 1.723960e+01,\n",
              "       1.164900e+00, 6.905600e+00, 1.197240e+01, 1.616100e+00,\n",
              "       5.385900e+00, 1.683700e+00, 1.884500e+00, 5.320100e+00,\n",
              "       1.967200e+00, 4.923600e+00, 2.000510e+01, 2.925800e+00,\n",
              "       8.769000e-01, 1.759600e+00, 1.670800e+00, 2.679500e+01,\n",
              "       1.278200e+00, 3.624100e+00, 3.731300e+00, 2.932200e+00,\n",
              "       2.040000e+00, 2.631700e+00, 2.041800e+00, 8.632000e-01,\n",
              "       1.519900e+00, 5.130400e+00, 6.585900e+00, 5.208600e+01,\n",
              "       3.470900e+00, 9.190800e+00, 4.619400e+00, 9.610000e-01,\n",
              "       6.121300e+00, 1.880200e+00, 1.169700e+00, 7.844500e+00,\n",
              "       4.890300e+00, 7.125000e-01, 8.735000e-01, 2.413400e+00,\n",
              "       1.126800e+00, 5.465000e-01, 3.001500e+00, 1.579100e+00,\n",
              "       5.405000e-01, 2.460200e+00, 1.233000e+00, 1.524100e+00,\n",
              "       1.457020e+01, 6.407000e-01, 3.252000e+00, 2.792600e+00,\n",
              "       5.103000e-01, 1.175480e+01, 1.018900e+00, 2.060800e+00,\n",
              "       3.964700e+00, 8.997000e-01, 9.040000e-01, 9.279000e-01,\n",
              "       1.574360e+01, 2.244000e+00, 1.425500e+00, 1.897000e+00,\n",
              "       1.637000e+01, 8.668000e-01, 1.402360e+01, 3.100900e+00,\n",
              "       9.204000e-01, 1.338000e+00, 2.947000e-01, 7.737000e-01,\n",
              "       5.771300e+00, 2.415600e+00, 2.680000e-01, 1.323500e+00,\n",
              "       5.119300e+00, 3.450400e+00, 1.572200e+00, 4.008800e+00,\n",
              "       4.659000e-01, 2.081200e+00, 6.668600e+00, 3.440300e+00,\n",
              "       1.111690e+01, 1.418700e+00, 2.033000e+00, 4.911500e+00,\n",
              "       3.473200e+00, 3.982800e+00, 8.322300e+00, 2.459700e+00,\n",
              "       2.913660e+01, 1.302400e+00, 2.928700e+00, 1.793700e+00,\n",
              "       8.490500e+00, 4.589100e+00, 1.935200e+00, 1.257500e+01,\n",
              "       1.734900e+00, 7.182800e+00, 2.330200e+00, 3.716000e+00,\n",
              "       1.316400e+00, 3.355000e+00, 8.833800e+00, 1.795200e+00,\n",
              "       7.026100e+00, 1.036930e+01, 3.630600e+00, 4.284000e+00,\n",
              "       8.785900e+00, 1.835200e+00, 1.553600e+00, 5.947000e-01,\n",
              "       1.219950e+01, 3.110300e+00, 5.894200e+00, 3.897200e+00,\n",
              "       3.433800e+00, 1.692500e+00, 1.289170e+01, 6.104100e+00,\n",
              "       2.018000e+01, 7.806000e-01, 2.042000e+00, 3.132800e+00,\n",
              "       1.545450e+01, 1.128000e+00, 1.863800e+00, 3.900000e-01,\n",
              "       2.652000e+00, 1.744900e+00, 1.196900e+00, 1.617660e+01,\n",
              "       9.544000e-01, 8.171000e+00, 3.118200e+00, 1.541800e+00,\n",
              "       3.933570e+01, 5.275000e+00, 1.010100e+00, 2.058800e+00,\n",
              "       7.020200e+00, 2.075310e+01, 4.277900e+00, 1.832400e+00,\n",
              "       2.071000e+00, 4.741400e+00, 6.275000e-01, 2.670400e+00,\n",
              "       2.555900e+00, 2.824700e+00, 2.780910e+01, 1.051400e+00,\n",
              "       1.347270e+01, 1.658740e+01, 2.962300e+00, 7.824000e-01,\n",
              "       3.974600e+00, 1.938700e+00, 4.186000e+00, 2.521200e+01,\n",
              "       1.932400e+00, 1.504762e+02, 2.476800e+00, 1.056210e+01,\n",
              "       2.613000e+00, 7.108800e+00, 2.040100e+00, 3.458700e+00,\n",
              "       3.252400e+00, 3.693000e-01, 3.008800e+00, 1.422200e+00,\n",
              "       5.112400e+00, 5.093200e+00, 4.573600e+00, 3.965300e+00,\n",
              "       5.834000e-01, 8.920500e+00, 1.377600e+00, 5.457000e-01,\n",
              "       5.421900e+00, 3.989000e-01, 3.052800e+00, 5.342100e+00,\n",
              "       9.276000e-01, 1.517320e+01, 7.141000e-01, 3.246000e-01,\n",
              "       7.882800e+00, 2.782200e+00, 2.575800e+00, 8.851900e+00,\n",
              "       2.532200e+00, 8.385500e+00, 4.986100e+00, 2.464500e+00,\n",
              "       4.087500e+00, 7.901000e-01, 2.705400e+00, 2.220100e+01,\n",
              "       1.136610e+01, 1.341000e+00, 3.011000e-01, 2.708800e+00,\n",
              "       1.843600e+00, 1.064120e+01, 7.000000e-01, 1.224700e+00,\n",
              "       8.867800e+00, 2.095213e+02, 5.993700e+00, 4.406600e+00,\n",
              "       7.023400e+00, 6.979200e+00, 1.093600e+00, 1.122200e+00,\n",
              "       3.276600e+00, 1.066340e+01, 4.899400e+00, 7.189300e+00,\n",
              "       1.988000e-01, 1.526800e+00, 1.697360e+01, 8.594200e+00,\n",
              "       2.654000e+00, 2.173700e+00, 6.686170e+01, 3.437400e+00,\n",
              "       5.517100e+00, 3.023700e+00, 5.697200e+00, 9.280400e+00,\n",
              "       4.510400e+00, 5.441000e+00, 2.116000e+00, 3.199900e+00,\n",
              "       6.705800e+00, 7.324400e+00, 6.662500e+00, 1.936000e+00,\n",
              "       2.869260e+01, 2.352700e+00, 1.000000e-02, 4.564300e+00,\n",
              "       1.412800e+00, 5.296800e+00, 3.386900e+00, 1.336700e+00,\n",
              "       1.774650e+01, 2.835000e+00, 5.110000e-01, 1.592300e+00,\n",
              "       4.288600e+00, 8.488000e-01, 4.227100e+00, 8.029400e+00,\n",
              "       6.313200e+00, 3.223400e+00, 5.152000e-01, 3.109000e-01,\n",
              "       1.008450e+01, 2.775800e+00, 2.497100e+00, 7.942400e+00,\n",
              "       4.512450e+01, 1.197120e+01, 1.455600e+00, 3.450100e+00,\n",
              "       8.993400e+00, 2.149700e+00, 7.512000e+00, 1.059370e+01,\n",
              "       1.200700e+00, 2.802200e+00, 2.506600e+01, 9.206100e+00,\n",
              "       3.007400e+00, 7.766400e+00, 2.836000e-01, 1.535200e+00,\n",
              "       1.025450e+01, 4.502200e+00, 1.030770e+01, 3.193000e+00,\n",
              "       8.574000e-01, 8.415000e-01, 3.456600e+00, 4.135000e-01,\n",
              "       1.027600e+00, 1.878380e+01, 9.571000e-01, 2.735200e+00,\n",
              "       2.127200e+00, 3.872300e+00, 1.541000e+00, 3.711400e+00,\n",
              "       6.632500e+00, 2.668310e+01, 1.098400e+00, 2.856000e+00,\n",
              "       9.488000e-01, 2.485100e+00, 3.246000e+00, 2.269800e+00,\n",
              "       2.707500e+00, 2.344800e+00, 4.450000e+00, 6.819000e-01,\n",
              "       4.545400e+00, 7.623000e-01, 2.014200e+00, 4.364200e+00,\n",
              "       5.423000e-01, 5.734800e+00, 2.946600e+00, 1.157100e+00,\n",
              "       9.292000e-01, 5.222400e+00, 1.393500e+00, 2.431800e+00,\n",
              "       6.426000e-01, 2.156202e+02, 2.898800e+00, 1.731300e+01,\n",
              "       2.493300e+00, 2.922100e+00, 2.143400e+00, 4.868800e+00,\n",
              "       2.674000e+00, 1.270070e+01, 2.914300e+00, 1.414100e+00,\n",
              "       2.981200e+00, 2.363100e+00, 1.064000e+00, 2.672000e+00,\n",
              "       2.219590e+01, 7.788000e-01, 2.520900e+00, 2.206600e+00,\n",
              "       3.272900e+00, 1.535400e+00, 6.455060e+01, 2.315000e+00,\n",
              "       2.078820e+01, 7.439200e+00, 1.806090e+01, 6.031900e+00,\n",
              "       6.003300e+00, 9.428000e-01, 1.288600e+00, 1.829710e+01,\n",
              "       1.719300e+00, 1.165940e+01, 6.883000e+00, 5.945000e-01,\n",
              "       1.898970e+01, 8.226000e-01, 1.646900e+00, 2.122660e+01,\n",
              "       2.300400e+00, 1.173300e+00, 4.180600e+00, 1.559430e+01,\n",
              "       3.195800e+00, 1.242400e+01, 2.805400e+00, 2.211800e+00,\n",
              "       6.479200e+00, 3.039400e+00, 2.883050e+01, 8.338900e+00,\n",
              "       1.854400e+00, 8.921000e-01, 5.095000e-01, 6.070600e+00,\n",
              "       3.828900e+00, 1.860700e+00, 6.841400e+00, 7.804000e+00,\n",
              "       2.236000e-01, 3.489000e-01, 3.039700e+00, 5.235770e+01,\n",
              "       3.142100e+00, 5.249500e+00, 6.134700e+00, 2.522700e+00,\n",
              "       1.120200e+00, 4.101000e-01, 2.247700e+00, 2.711400e+00,\n",
              "       2.186900e+00, 4.925700e+00, 1.510300e+00, 1.067310e+01,\n",
              "       9.555900e+00, 4.120830e+01, 5.012300e+00, 1.472000e+00,\n",
              "       1.666080e+01, 5.996200e+00, 2.136200e+00, 2.465900e+00,\n",
              "       1.689300e+00, 2.317240e+01, 1.990400e+01, 2.539600e+00,\n",
              "       1.129300e+00, 1.975600e+00, 9.097500e+00, 2.242100e+00,\n",
              "       6.205100e+00, 8.646000e-01, 7.092000e-01, 1.129600e+00,\n",
              "       3.449300e+00, 7.517100e+00, 2.356500e+00, 4.839200e+00,\n",
              "       3.684600e+00, 1.442840e+01, 1.463300e+00, 7.496000e-01,\n",
              "       1.335510e+01, 1.153830e+01, 9.931300e+00, 1.290600e+01,\n",
              "       3.699000e-01, 2.701000e+00, 2.627800e+00, 3.258700e+00,\n",
              "       5.206100e+00, 7.099000e+00, 7.795500e+00, 2.133200e+00,\n",
              "       1.982400e+00, 3.715280e+01, 6.568600e+00, 5.604300e+00],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3TWpN0nVTpUx"
      },
      "source": [
        "## Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WQ1FKEs-4btX"
      },
      "source": [
        "### Normalize data\n",
        "- Normalize features\n",
        "- Use tf.math.l2_normalize to normalize features\n",
        "- You can read more about it here https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V0Tfe00X78wB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "fab7ed95-a145-45b5-d85b-0c6c9d86855b"
      },
      "source": [
        "tf.math.l2_normalize(\n",
        "    X,\n",
        "    axis=None,\n",
        "    epsilon=1e-12,\n",
        "    name=None\n",
        ")"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1000, 4), dtype=float32, numpy=\n",
              "array([[0.02202894, 0.02245906, 0.02182905, 0.02253223],\n",
              "       [0.02235197, 0.02141321, 0.02140607, 0.02240551],\n",
              "       [0.0207707 , 0.02051548, 0.02051192, 0.02137037],\n",
              "       ...,\n",
              "       [0.00505436, 0.00513467, 0.00499903, 0.00514181],\n",
              "       [0.00785282, 0.0079956 , 0.0078082 , 0.00799738],\n",
              "       [0.00643931, 0.00662849, 0.00642682, 0.00664455]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wmXUGc2oTspa"
      },
      "source": [
        "## Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VJelDMpzxs0L"
      },
      "source": [
        "### Define weight and bias\n",
        "- Initialize weight and bias with tf.zeros\n",
        "- tf.zeros is an initializer that generates tensors initialized to 0\n",
        "- Specify the value for shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8o9RPWVTxs0O",
        "colab": {}
      },
      "source": [
        "W = tf.zeros(shape=(4, 1))\n",
        "b = tf.zeros(shape=(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8a0wr94aTyjg"
      },
      "source": [
        "## Question 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zMXXYdOSxs0Q"
      },
      "source": [
        "### Get prediction\n",
        "- Define a function to get prediction\n",
        "- Approach: prediction = (X * W) + b; here is X is features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U8Cty1y0xs0S",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def prediction(features, W, b):\n",
        "    y_pred = tf.add(tf.matmul(features, W), b)\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lQmS3Tauxs0V"
      },
      "source": [
        "### Calculate loss\n",
        "- Calculate loss using predictions\n",
        "- Define a function to calculate loss\n",
        "- We are calculating mean squared error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-FRXmDd5xs0X",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def loss(y_actual, y_predicted):\n",
        "    diff = y_actual - y_predicted\n",
        "    sqr = tf.square(diff)\n",
        "    avg = tf.reduce_mean(sqr)\n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZbBpnOtfT0wd"
      },
      "source": [
        "## Question 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bkOzAUUsTmF_"
      },
      "source": [
        "### Define a function to train the model\n",
        "1.   Record all the mathematical steps to calculate Loss\n",
        "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
        "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2R4uieGYLYtM",
        "colab": {}
      },
      "source": [
        "def train(x, y_actual, w, b, learning_rate=0.01):\n",
        "    \n",
        "    # Record mathematical operations on 'tape' to calculate loss\n",
        "    with tf.GradientTape() as t:\n",
        "        t.watch([w,b])\n",
        "        current_prediction = prediction(x, w, b)\n",
        "        current_loss = loss(y_actual, current_prediction)\n",
        "    \n",
        "    # Calculate Gradients for Loss with respect to Weights and Bias\n",
        "    dw, db = t.gradient(current_loss,[w, b])\n",
        "    \n",
        "    # Update Weights and Bias\n",
        "    w = w - learning_rate * dw\n",
        "    b = b - learning_rate * db\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AW4SEP8kT2ls"
      },
      "source": [
        "## Question 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yeN0deOvT81N"
      },
      "source": [
        "### Train the model for 100 epochs \n",
        "- Observe the training loss at every iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jjkn4gUgLevE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c04bda3-cfdb-4a6c-a936-8de9a2806f72"
      },
      "source": [
        "for i in range(100):    \n",
        "    W, b = train(X, Y, W, b)\n",
        "    print('Current Training Loss on iteration', i, loss(Y, prediction(X, W, b)).numpy())"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Training Loss on iteration 0 6288697.0\n",
            "Current Training Loss on iteration 1 2471274000000.0\n",
            "Current Training Loss on iteration 2 9.711742e+17\n",
            "Current Training Loss on iteration 3 3.8165654e+23\n",
            "Current Training Loss on iteration 4 1.4998504e+29\n",
            "Current Training Loss on iteration 5 inf\n",
            "Current Training Loss on iteration 6 inf\n",
            "Current Training Loss on iteration 7 inf\n",
            "Current Training Loss on iteration 8 inf\n",
            "Current Training Loss on iteration 9 inf\n",
            "Current Training Loss on iteration 10 inf\n",
            "Current Training Loss on iteration 11 inf\n",
            "Current Training Loss on iteration 12 inf\n",
            "Current Training Loss on iteration 13 inf\n",
            "Current Training Loss on iteration 14 nan\n",
            "Current Training Loss on iteration 15 nan\n",
            "Current Training Loss on iteration 16 nan\n",
            "Current Training Loss on iteration 17 nan\n",
            "Current Training Loss on iteration 18 nan\n",
            "Current Training Loss on iteration 19 nan\n",
            "Current Training Loss on iteration 20 nan\n",
            "Current Training Loss on iteration 21 nan\n",
            "Current Training Loss on iteration 22 nan\n",
            "Current Training Loss on iteration 23 nan\n",
            "Current Training Loss on iteration 24 nan\n",
            "Current Training Loss on iteration 25 nan\n",
            "Current Training Loss on iteration 26 nan\n",
            "Current Training Loss on iteration 27 nan\n",
            "Current Training Loss on iteration 28 nan\n",
            "Current Training Loss on iteration 29 nan\n",
            "Current Training Loss on iteration 30 nan\n",
            "Current Training Loss on iteration 31 nan\n",
            "Current Training Loss on iteration 32 nan\n",
            "Current Training Loss on iteration 33 nan\n",
            "Current Training Loss on iteration 34 nan\n",
            "Current Training Loss on iteration 35 nan\n",
            "Current Training Loss on iteration 36 nan\n",
            "Current Training Loss on iteration 37 nan\n",
            "Current Training Loss on iteration 38 nan\n",
            "Current Training Loss on iteration 39 nan\n",
            "Current Training Loss on iteration 40 nan\n",
            "Current Training Loss on iteration 41 nan\n",
            "Current Training Loss on iteration 42 nan\n",
            "Current Training Loss on iteration 43 nan\n",
            "Current Training Loss on iteration 44 nan\n",
            "Current Training Loss on iteration 45 nan\n",
            "Current Training Loss on iteration 46 nan\n",
            "Current Training Loss on iteration 47 nan\n",
            "Current Training Loss on iteration 48 nan\n",
            "Current Training Loss on iteration 49 nan\n",
            "Current Training Loss on iteration 50 nan\n",
            "Current Training Loss on iteration 51 nan\n",
            "Current Training Loss on iteration 52 nan\n",
            "Current Training Loss on iteration 53 nan\n",
            "Current Training Loss on iteration 54 nan\n",
            "Current Training Loss on iteration 55 nan\n",
            "Current Training Loss on iteration 56 nan\n",
            "Current Training Loss on iteration 57 nan\n",
            "Current Training Loss on iteration 58 nan\n",
            "Current Training Loss on iteration 59 nan\n",
            "Current Training Loss on iteration 60 nan\n",
            "Current Training Loss on iteration 61 nan\n",
            "Current Training Loss on iteration 62 nan\n",
            "Current Training Loss on iteration 63 nan\n",
            "Current Training Loss on iteration 64 nan\n",
            "Current Training Loss on iteration 65 nan\n",
            "Current Training Loss on iteration 66 nan\n",
            "Current Training Loss on iteration 67 nan\n",
            "Current Training Loss on iteration 68 nan\n",
            "Current Training Loss on iteration 69 nan\n",
            "Current Training Loss on iteration 70 nan\n",
            "Current Training Loss on iteration 71 nan\n",
            "Current Training Loss on iteration 72 nan\n",
            "Current Training Loss on iteration 73 nan\n",
            "Current Training Loss on iteration 74 nan\n",
            "Current Training Loss on iteration 75 nan\n",
            "Current Training Loss on iteration 76 nan\n",
            "Current Training Loss on iteration 77 nan\n",
            "Current Training Loss on iteration 78 nan\n",
            "Current Training Loss on iteration 79 nan\n",
            "Current Training Loss on iteration 80 nan\n",
            "Current Training Loss on iteration 81 nan\n",
            "Current Training Loss on iteration 82 nan\n",
            "Current Training Loss on iteration 83 nan\n",
            "Current Training Loss on iteration 84 nan\n",
            "Current Training Loss on iteration 85 nan\n",
            "Current Training Loss on iteration 86 nan\n",
            "Current Training Loss on iteration 87 nan\n",
            "Current Training Loss on iteration 88 nan\n",
            "Current Training Loss on iteration 89 nan\n",
            "Current Training Loss on iteration 90 nan\n",
            "Current Training Loss on iteration 91 nan\n",
            "Current Training Loss on iteration 92 nan\n",
            "Current Training Loss on iteration 93 nan\n",
            "Current Training Loss on iteration 94 nan\n",
            "Current Training Loss on iteration 95 nan\n",
            "Current Training Loss on iteration 96 nan\n",
            "Current Training Loss on iteration 97 nan\n",
            "Current Training Loss on iteration 98 nan\n",
            "Current Training Loss on iteration 99 nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vanvD93FV0_k"
      },
      "source": [
        "### Observe values of Weight\n",
        "- Print the updated values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QSqpy4gtWaOD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8892cdae-81ac-46da-8da0-f7067d9310f8"
      },
      "source": [
        "print(W)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]], shape=(4, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y9KpRupYUEwy"
      },
      "source": [
        "### Observe values of Bias\n",
        "- Print the updated values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bhEWkGqHWohg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "824e092a-32a0-4c5b-fb9b-66f6c823ae5b"
      },
      "source": [
        "print(b)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([nan], shape=(1,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}