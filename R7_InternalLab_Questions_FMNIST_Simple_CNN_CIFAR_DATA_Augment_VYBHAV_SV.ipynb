{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R7_InternalLab_Questions_FMNIST_Simple_CNN_CIFAR_DATA_Augment_VYBHAV_SV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreatLearningAIML1/bangalore-aug19-batch-vybhavsv/blob/master/R7_InternalLab_Questions_FMNIST_Simple_CNN_CIFAR_DATA_Augment_VYBHAV_SV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyfMmMnPJjvn",
        "colab_type": "text"
      },
      "source": [
        "## Train a simple convnet on the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjcGOJhcJjvp",
        "colab_type": "text"
      },
      "source": [
        "In this, we will see how to deal with image data and train a convnet for image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR0Pl2XjJjvq",
        "colab_type": "text"
      },
      "source": [
        "### Load the  `fashion_mnist`  dataset\n",
        "\n",
        "** Use keras.datasets to load the dataset **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr75v_UYJjvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "35b9b45f-eb3b-47b9-d2e5-f2ed1d759ee4"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTI42-0qJjvw",
        "colab_type": "text"
      },
      "source": [
        "### Find no.of samples are there in training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2sf67VoJjvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "7ed2d262-e81c-4deb-9d0d-96a97c4be560"
      },
      "source": [
        "print(x_train, y_train)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]] [9 0 0 ... 3 0 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zewyDcBlJjv1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "827a7494-279c-4ea7-ea1c-93e4bc455535"
      },
      "source": [
        "print(x_test, y_test)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]] [9 2 1 ... 8 1 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WytT2eRnJjv4",
        "colab_type": "text"
      },
      "source": [
        "### Find dimensions of an image in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XycQGBSGJjv5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c4ab8688-8c87-4b2b-99ed-99275660c9fe"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jtdZ7RqJjv8",
        "colab_type": "text"
      },
      "source": [
        "### Convert train and test labels to one hot vectors\n",
        "\n",
        "** check `keras.utils.to_categorical()` **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIljJ5gNKZjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f85ec5a2-38c9-403a-be3f-a572df8c8bf2"
      },
      "source": [
        "import numpy as np\n",
        "print(np.unique(y_train))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAD3q5I6Jjv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LCGXs8dLWz3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ead37619-559c-4795-d917-6a3bf2f05251"
      },
      "source": [
        "print(y_train[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO5BRBzBJjwD",
        "colab_type": "text"
      },
      "source": [
        "### Normalize both the train and test image data from 0-255 to 0-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fUQpMHxJjwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okwo_SB5JjwI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a29d83f8-5247-40ee-f627-a6e19b7ea981"
      },
      "source": [
        "#Normalizing the input\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s8fn3mQLMqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8fb068e4-e959-4f2e-d420-e93af84fff7e"
      },
      "source": [
        "x_train[1,]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.16078432, 0.7372549 , 0.40392157, 0.21176471, 0.1882353 ,\n",
              "        0.16862746, 0.34117648, 0.65882355, 0.52156866, 0.0627451 ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.00392157, 0.        ,\n",
              "        0.        , 0.        , 0.19215687, 0.53333336, 0.85882354,\n",
              "        0.84705883, 0.89411765, 0.9254902 , 1.        , 1.        ,\n",
              "        1.        , 1.        , 0.8509804 , 0.84313726, 0.99607843,\n",
              "        0.90588236, 0.627451  , 0.1764706 , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.05490196, 0.6901961 , 0.87058824, 0.8784314 , 0.83137256,\n",
              "        0.79607844, 0.7764706 , 0.76862746, 0.78431374, 0.84313726,\n",
              "        0.8       , 0.7921569 , 0.7882353 , 0.7882353 , 0.7882353 ,\n",
              "        0.81960785, 0.85490197, 0.8784314 , 0.6431373 , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.7372549 , 0.85882354, 0.78431374, 0.7764706 , 0.7921569 ,\n",
              "        0.7764706 , 0.78039217, 0.78039217, 0.7882353 , 0.76862746,\n",
              "        0.7764706 , 0.7764706 , 0.78431374, 0.78431374, 0.78431374,\n",
              "        0.78431374, 0.7882353 , 0.78431374, 0.88235295, 0.16078432,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.2       ,\n",
              "        0.85882354, 0.78039217, 0.79607844, 0.79607844, 0.83137256,\n",
              "        0.93333334, 0.972549  , 0.98039216, 0.9607843 , 0.9764706 ,\n",
              "        0.9647059 , 0.96862745, 0.9882353 , 0.972549  , 0.92156863,\n",
              "        0.8117647 , 0.79607844, 0.79607844, 0.87058824, 0.54901963,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.45490196,\n",
              "        0.8862745 , 0.80784315, 0.8       , 0.8117647 , 0.8       ,\n",
              "        0.39607844, 0.29411766, 0.18431373, 0.28627452, 0.1882353 ,\n",
              "        0.19607843, 0.1764706 , 0.2       , 0.24705882, 0.44313726,\n",
              "        0.87058824, 0.7921569 , 0.80784315, 0.8627451 , 0.8784314 ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.78431374,\n",
              "        0.87058824, 0.81960785, 0.79607844, 0.84313726, 0.78431374,\n",
              "        0.        , 0.27450982, 0.38431373, 0.        , 0.40392157,\n",
              "        0.23137255, 0.26666668, 0.2784314 , 0.19215687, 0.        ,\n",
              "        0.85882354, 0.80784315, 0.8392157 , 0.8235294 , 0.98039216,\n",
              "        0.14901961, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.96862745,\n",
              "        0.85490197, 0.83137256, 0.8235294 , 0.84313726, 0.8392157 ,\n",
              "        0.        , 0.99607843, 0.9529412 , 0.54509807, 1.        ,\n",
              "        0.68235296, 0.9843137 , 1.        , 0.8039216 , 0.        ,\n",
              "        0.84313726, 0.8509804 , 0.8392157 , 0.8156863 , 0.8627451 ,\n",
              "        0.37254903, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.1764706 , 0.8862745 ,\n",
              "        0.8392157 , 0.8392157 , 0.84313726, 0.8784314 , 0.8039216 ,\n",
              "        0.        , 0.16470589, 0.13725491, 0.23529412, 0.0627451 ,\n",
              "        0.06666667, 0.04705882, 0.05098039, 0.27450982, 0.        ,\n",
              "        0.7411765 , 0.84705883, 0.83137256, 0.80784315, 0.83137256,\n",
              "        0.6117647 , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.6431373 , 0.92156863,\n",
              "        0.8392157 , 0.827451  , 0.8627451 , 0.84705883, 0.7882353 ,\n",
              "        0.20392157, 0.2784314 , 0.34901962, 0.36862746, 0.3254902 ,\n",
              "        0.30588236, 0.27450982, 0.29803923, 0.36078432, 0.34117648,\n",
              "        0.80784315, 0.8117647 , 0.87058824, 0.8352941 , 0.85882354,\n",
              "        0.8156863 , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.41568628, 0.73333335,\n",
              "        0.8745098 , 0.92941177, 0.972549  , 0.827451  , 0.7764706 ,\n",
              "        0.9882353 , 0.98039216, 0.972549  , 0.9607843 , 0.972549  ,\n",
              "        0.9882353 , 0.99215686, 0.98039216, 0.9882353 , 0.9372549 ,\n",
              "        0.7882353 , 0.83137256, 0.88235295, 0.84313726, 0.75686276,\n",
              "        0.44313726, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.06666667, 0.21176471, 0.62352943, 0.87058824, 0.75686276,\n",
              "        0.8156863 , 0.7529412 , 0.77254903, 0.78431374, 0.78431374,\n",
              "        0.78431374, 0.78431374, 0.7882353 , 0.79607844, 0.7647059 ,\n",
              "        0.8235294 , 0.64705884, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.18431373, 0.88235295, 0.7529412 ,\n",
              "        0.8392157 , 0.79607844, 0.80784315, 0.8       , 0.8       ,\n",
              "        0.8039216 , 0.80784315, 0.8       , 0.83137256, 0.77254903,\n",
              "        0.85490197, 0.41960785, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "        0.02352941, 0.        , 0.18039216, 0.83137256, 0.7647059 ,\n",
              "        0.83137256, 0.7921569 , 0.80784315, 0.8039216 , 0.8       ,\n",
              "        0.8039216 , 0.80784315, 0.8       , 0.83137256, 0.78431374,\n",
              "        0.85490197, 0.35686275, 0.        , 0.01176471, 0.00392157,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.        , 0.04313726, 0.77254903, 0.78039217,\n",
              "        0.8039216 , 0.7921569 , 0.8039216 , 0.80784315, 0.8       ,\n",
              "        0.8039216 , 0.8117647 , 0.8       , 0.8039216 , 0.8039216 ,\n",
              "        0.85490197, 0.3019608 , 0.        , 0.01960784, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.01176471, 0.        , 0.00784314, 0.7490196 , 0.7764706 ,\n",
              "        0.7882353 , 0.8039216 , 0.80784315, 0.8039216 , 0.8039216 ,\n",
              "        0.80784315, 0.81960785, 0.80784315, 0.78039217, 0.81960785,\n",
              "        0.85882354, 0.2901961 , 0.        , 0.01960784, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00784314, 0.        , 0.        , 0.7372549 , 0.77254903,\n",
              "        0.78431374, 0.8117647 , 0.8117647 , 0.8       , 0.8117647 ,\n",
              "        0.8117647 , 0.8235294 , 0.8156863 , 0.7764706 , 0.8117647 ,\n",
              "        0.8666667 , 0.28235295, 0.        , 0.01568628, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00784314, 0.        , 0.        , 0.84313726, 0.7764706 ,\n",
              "        0.79607844, 0.80784315, 0.8156863 , 0.8039216 , 0.8117647 ,\n",
              "        0.8117647 , 0.8235294 , 0.8156863 , 0.78431374, 0.7921569 ,\n",
              "        0.87058824, 0.29411766, 0.        , 0.01568628, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.        , 0.        , 0.83137256, 0.7764706 ,\n",
              "        0.81960785, 0.80784315, 0.81960785, 0.80784315, 0.8156863 ,\n",
              "        0.8117647 , 0.827451  , 0.80784315, 0.8039216 , 0.7764706 ,\n",
              "        0.8666667 , 0.3137255 , 0.        , 0.01176471, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.        , 0.        , 0.8       , 0.7882353 ,\n",
              "        0.8039216 , 0.8156863 , 0.8117647 , 0.8039216 , 0.827451  ,\n",
              "        0.8039216 , 0.8235294 , 0.8235294 , 0.81960785, 0.7647059 ,\n",
              "        0.8666667 , 0.3764706 , 0.        , 0.01176471, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.        , 0.        , 0.7921569 , 0.7882353 ,\n",
              "        0.8039216 , 0.81960785, 0.8117647 , 0.8039216 , 0.8352941 ,\n",
              "        0.80784315, 0.8235294 , 0.81960785, 0.8235294 , 0.7607843 ,\n",
              "        0.8509804 , 0.4117647 , 0.        , 0.00784314, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.        , 0.        , 0.8       , 0.8       ,\n",
              "        0.8039216 , 0.8156863 , 0.8117647 , 0.8039216 , 0.84313726,\n",
              "        0.8117647 , 0.8235294 , 0.8156863 , 0.827451  , 0.75686276,\n",
              "        0.8352941 , 0.4509804 , 0.        , 0.00784314, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.8       , 0.8117647 ,\n",
              "        0.8117647 , 0.8156863 , 0.80784315, 0.80784315, 0.84313726,\n",
              "        0.8235294 , 0.8235294 , 0.8117647 , 0.83137256, 0.7647059 ,\n",
              "        0.8235294 , 0.4627451 , 0.        , 0.00784314, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.        , 0.        , 0.7764706 , 0.8156863 ,\n",
              "        0.8156863 , 0.8156863 , 0.8       , 0.8117647 , 0.83137256,\n",
              "        0.83137256, 0.8235294 , 0.8117647 , 0.827451  , 0.76862746,\n",
              "        0.8117647 , 0.4745098 , 0.        , 0.00392157, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.        , 0.        , 0.7764706 , 0.8235294 ,\n",
              "        0.8117647 , 0.8156863 , 0.80784315, 0.81960785, 0.8352941 ,\n",
              "        0.83137256, 0.827451  , 0.8117647 , 0.8235294 , 0.77254903,\n",
              "        0.8117647 , 0.4862745 , 0.        , 0.00392157, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.6745098 , 0.8235294 ,\n",
              "        0.79607844, 0.7882353 , 0.78039217, 0.8       , 0.8117647 ,\n",
              "        0.8039216 , 0.8       , 0.7882353 , 0.8039216 , 0.77254903,\n",
              "        0.80784315, 0.49803922, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.7372549 , 0.8666667 ,\n",
              "        0.8392157 , 0.91764706, 0.9254902 , 0.93333334, 0.95686275,\n",
              "        0.95686275, 0.95686275, 0.9411765 , 0.9529412 , 0.8392157 ,\n",
              "        0.8784314 , 0.63529414, 0.        , 0.00784314, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.        , 0.        , 0.54509807, 0.57254905,\n",
              "        0.50980395, 0.5294118 , 0.5294118 , 0.5372549 , 0.49019608,\n",
              "        0.4862745 , 0.49019608, 0.4745098 , 0.46666667, 0.44705883,\n",
              "        0.50980395, 0.29803923, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5-DwgrJjwM",
        "colab_type": "text"
      },
      "source": [
        "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPGVQ-JJJjwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL3OQ3QHNCyE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9cf3b519-ee6b-47d0-b257-e7a2a0c0f93c"
      },
      "source": [
        "x_train[1,]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.16078432],\n",
              "        [0.7372549 ],\n",
              "        [0.40392157],\n",
              "        [0.21176471],\n",
              "        [0.1882353 ],\n",
              "        [0.16862746],\n",
              "        [0.34117648],\n",
              "        [0.65882355],\n",
              "        [0.52156866],\n",
              "        [0.0627451 ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.19215687],\n",
              "        [0.53333336],\n",
              "        [0.85882354],\n",
              "        [0.84705883],\n",
              "        [0.89411765],\n",
              "        [0.9254902 ],\n",
              "        [1.        ],\n",
              "        [1.        ],\n",
              "        [1.        ],\n",
              "        [1.        ],\n",
              "        [0.8509804 ],\n",
              "        [0.84313726],\n",
              "        [0.99607843],\n",
              "        [0.90588236],\n",
              "        [0.627451  ],\n",
              "        [0.1764706 ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.05490196],\n",
              "        [0.6901961 ],\n",
              "        [0.87058824],\n",
              "        [0.8784314 ],\n",
              "        [0.83137256],\n",
              "        [0.79607844],\n",
              "        [0.7764706 ],\n",
              "        [0.76862746],\n",
              "        [0.78431374],\n",
              "        [0.84313726],\n",
              "        [0.8       ],\n",
              "        [0.7921569 ],\n",
              "        [0.7882353 ],\n",
              "        [0.7882353 ],\n",
              "        [0.7882353 ],\n",
              "        [0.81960785],\n",
              "        [0.85490197],\n",
              "        [0.8784314 ],\n",
              "        [0.6431373 ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.7372549 ],\n",
              "        [0.85882354],\n",
              "        [0.78431374],\n",
              "        [0.7764706 ],\n",
              "        [0.7921569 ],\n",
              "        [0.7764706 ],\n",
              "        [0.78039217],\n",
              "        [0.78039217],\n",
              "        [0.7882353 ],\n",
              "        [0.76862746],\n",
              "        [0.7764706 ],\n",
              "        [0.7764706 ],\n",
              "        [0.78431374],\n",
              "        [0.78431374],\n",
              "        [0.78431374],\n",
              "        [0.78431374],\n",
              "        [0.7882353 ],\n",
              "        [0.78431374],\n",
              "        [0.88235295],\n",
              "        [0.16078432],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.2       ],\n",
              "        [0.85882354],\n",
              "        [0.78039217],\n",
              "        [0.79607844],\n",
              "        [0.79607844],\n",
              "        [0.83137256],\n",
              "        [0.93333334],\n",
              "        [0.972549  ],\n",
              "        [0.98039216],\n",
              "        [0.9607843 ],\n",
              "        [0.9764706 ],\n",
              "        [0.9647059 ],\n",
              "        [0.96862745],\n",
              "        [0.9882353 ],\n",
              "        [0.972549  ],\n",
              "        [0.92156863],\n",
              "        [0.8117647 ],\n",
              "        [0.79607844],\n",
              "        [0.79607844],\n",
              "        [0.87058824],\n",
              "        [0.54901963],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.45490196],\n",
              "        [0.8862745 ],\n",
              "        [0.80784315],\n",
              "        [0.8       ],\n",
              "        [0.8117647 ],\n",
              "        [0.8       ],\n",
              "        [0.39607844],\n",
              "        [0.29411766],\n",
              "        [0.18431373],\n",
              "        [0.28627452],\n",
              "        [0.1882353 ],\n",
              "        [0.19607843],\n",
              "        [0.1764706 ],\n",
              "        [0.2       ],\n",
              "        [0.24705882],\n",
              "        [0.44313726],\n",
              "        [0.87058824],\n",
              "        [0.7921569 ],\n",
              "        [0.80784315],\n",
              "        [0.8627451 ],\n",
              "        [0.8784314 ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.78431374],\n",
              "        [0.87058824],\n",
              "        [0.81960785],\n",
              "        [0.79607844],\n",
              "        [0.84313726],\n",
              "        [0.78431374],\n",
              "        [0.        ],\n",
              "        [0.27450982],\n",
              "        [0.38431373],\n",
              "        [0.        ],\n",
              "        [0.40392157],\n",
              "        [0.23137255],\n",
              "        [0.26666668],\n",
              "        [0.2784314 ],\n",
              "        [0.19215687],\n",
              "        [0.        ],\n",
              "        [0.85882354],\n",
              "        [0.80784315],\n",
              "        [0.8392157 ],\n",
              "        [0.8235294 ],\n",
              "        [0.98039216],\n",
              "        [0.14901961],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.96862745],\n",
              "        [0.85490197],\n",
              "        [0.83137256],\n",
              "        [0.8235294 ],\n",
              "        [0.84313726],\n",
              "        [0.8392157 ],\n",
              "        [0.        ],\n",
              "        [0.99607843],\n",
              "        [0.9529412 ],\n",
              "        [0.54509807],\n",
              "        [1.        ],\n",
              "        [0.68235296],\n",
              "        [0.9843137 ],\n",
              "        [1.        ],\n",
              "        [0.8039216 ],\n",
              "        [0.        ],\n",
              "        [0.84313726],\n",
              "        [0.8509804 ],\n",
              "        [0.8392157 ],\n",
              "        [0.8156863 ],\n",
              "        [0.8627451 ],\n",
              "        [0.37254903],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.1764706 ],\n",
              "        [0.8862745 ],\n",
              "        [0.8392157 ],\n",
              "        [0.8392157 ],\n",
              "        [0.84313726],\n",
              "        [0.8784314 ],\n",
              "        [0.8039216 ],\n",
              "        [0.        ],\n",
              "        [0.16470589],\n",
              "        [0.13725491],\n",
              "        [0.23529412],\n",
              "        [0.0627451 ],\n",
              "        [0.06666667],\n",
              "        [0.04705882],\n",
              "        [0.05098039],\n",
              "        [0.27450982],\n",
              "        [0.        ],\n",
              "        [0.7411765 ],\n",
              "        [0.84705883],\n",
              "        [0.83137256],\n",
              "        [0.80784315],\n",
              "        [0.83137256],\n",
              "        [0.6117647 ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.6431373 ],\n",
              "        [0.92156863],\n",
              "        [0.8392157 ],\n",
              "        [0.827451  ],\n",
              "        [0.8627451 ],\n",
              "        [0.84705883],\n",
              "        [0.7882353 ],\n",
              "        [0.20392157],\n",
              "        [0.2784314 ],\n",
              "        [0.34901962],\n",
              "        [0.36862746],\n",
              "        [0.3254902 ],\n",
              "        [0.30588236],\n",
              "        [0.27450982],\n",
              "        [0.29803923],\n",
              "        [0.36078432],\n",
              "        [0.34117648],\n",
              "        [0.80784315],\n",
              "        [0.8117647 ],\n",
              "        [0.87058824],\n",
              "        [0.8352941 ],\n",
              "        [0.85882354],\n",
              "        [0.8156863 ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.41568628],\n",
              "        [0.73333335],\n",
              "        [0.8745098 ],\n",
              "        [0.92941177],\n",
              "        [0.972549  ],\n",
              "        [0.827451  ],\n",
              "        [0.7764706 ],\n",
              "        [0.9882353 ],\n",
              "        [0.98039216],\n",
              "        [0.972549  ],\n",
              "        [0.9607843 ],\n",
              "        [0.972549  ],\n",
              "        [0.9882353 ],\n",
              "        [0.99215686],\n",
              "        [0.98039216],\n",
              "        [0.9882353 ],\n",
              "        [0.9372549 ],\n",
              "        [0.7882353 ],\n",
              "        [0.83137256],\n",
              "        [0.88235295],\n",
              "        [0.84313726],\n",
              "        [0.75686276],\n",
              "        [0.44313726],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.06666667],\n",
              "        [0.21176471],\n",
              "        [0.62352943],\n",
              "        [0.87058824],\n",
              "        [0.75686276],\n",
              "        [0.8156863 ],\n",
              "        [0.7529412 ],\n",
              "        [0.77254903],\n",
              "        [0.78431374],\n",
              "        [0.78431374],\n",
              "        [0.78431374],\n",
              "        [0.78431374],\n",
              "        [0.7882353 ],\n",
              "        [0.79607844],\n",
              "        [0.7647059 ],\n",
              "        [0.8235294 ],\n",
              "        [0.64705884],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.18431373],\n",
              "        [0.88235295],\n",
              "        [0.7529412 ],\n",
              "        [0.8392157 ],\n",
              "        [0.79607844],\n",
              "        [0.80784315],\n",
              "        [0.8       ],\n",
              "        [0.8       ],\n",
              "        [0.8039216 ],\n",
              "        [0.80784315],\n",
              "        [0.8       ],\n",
              "        [0.83137256],\n",
              "        [0.77254903],\n",
              "        [0.85490197],\n",
              "        [0.41960785],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00392157],\n",
              "        [0.02352941],\n",
              "        [0.        ],\n",
              "        [0.18039216],\n",
              "        [0.83137256],\n",
              "        [0.7647059 ],\n",
              "        [0.83137256],\n",
              "        [0.7921569 ],\n",
              "        [0.80784315],\n",
              "        [0.8039216 ],\n",
              "        [0.8       ],\n",
              "        [0.8039216 ],\n",
              "        [0.80784315],\n",
              "        [0.8       ],\n",
              "        [0.83137256],\n",
              "        [0.78431374],\n",
              "        [0.85490197],\n",
              "        [0.35686275],\n",
              "        [0.        ],\n",
              "        [0.01176471],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.04313726],\n",
              "        [0.77254903],\n",
              "        [0.78039217],\n",
              "        [0.8039216 ],\n",
              "        [0.7921569 ],\n",
              "        [0.8039216 ],\n",
              "        [0.80784315],\n",
              "        [0.8       ],\n",
              "        [0.8039216 ],\n",
              "        [0.8117647 ],\n",
              "        [0.8       ],\n",
              "        [0.8039216 ],\n",
              "        [0.8039216 ],\n",
              "        [0.85490197],\n",
              "        [0.3019608 ],\n",
              "        [0.        ],\n",
              "        [0.01960784],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.01176471],\n",
              "        [0.        ],\n",
              "        [0.00784314],\n",
              "        [0.7490196 ],\n",
              "        [0.7764706 ],\n",
              "        [0.7882353 ],\n",
              "        [0.8039216 ],\n",
              "        [0.80784315],\n",
              "        [0.8039216 ],\n",
              "        [0.8039216 ],\n",
              "        [0.80784315],\n",
              "        [0.81960785],\n",
              "        [0.80784315],\n",
              "        [0.78039217],\n",
              "        [0.81960785],\n",
              "        [0.85882354],\n",
              "        [0.2901961 ],\n",
              "        [0.        ],\n",
              "        [0.01960784],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00784314],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.7372549 ],\n",
              "        [0.77254903],\n",
              "        [0.78431374],\n",
              "        [0.8117647 ],\n",
              "        [0.8117647 ],\n",
              "        [0.8       ],\n",
              "        [0.8117647 ],\n",
              "        [0.8117647 ],\n",
              "        [0.8235294 ],\n",
              "        [0.8156863 ],\n",
              "        [0.7764706 ],\n",
              "        [0.8117647 ],\n",
              "        [0.8666667 ],\n",
              "        [0.28235295],\n",
              "        [0.        ],\n",
              "        [0.01568628],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00784314],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.84313726],\n",
              "        [0.7764706 ],\n",
              "        [0.79607844],\n",
              "        [0.80784315],\n",
              "        [0.8156863 ],\n",
              "        [0.8039216 ],\n",
              "        [0.8117647 ],\n",
              "        [0.8117647 ],\n",
              "        [0.8235294 ],\n",
              "        [0.8156863 ],\n",
              "        [0.78431374],\n",
              "        [0.7921569 ],\n",
              "        [0.87058824],\n",
              "        [0.29411766],\n",
              "        [0.        ],\n",
              "        [0.01568628],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.83137256],\n",
              "        [0.7764706 ],\n",
              "        [0.81960785],\n",
              "        [0.80784315],\n",
              "        [0.81960785],\n",
              "        [0.80784315],\n",
              "        [0.8156863 ],\n",
              "        [0.8117647 ],\n",
              "        [0.827451  ],\n",
              "        [0.80784315],\n",
              "        [0.8039216 ],\n",
              "        [0.7764706 ],\n",
              "        [0.8666667 ],\n",
              "        [0.3137255 ],\n",
              "        [0.        ],\n",
              "        [0.01176471],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.8       ],\n",
              "        [0.7882353 ],\n",
              "        [0.8039216 ],\n",
              "        [0.8156863 ],\n",
              "        [0.8117647 ],\n",
              "        [0.8039216 ],\n",
              "        [0.827451  ],\n",
              "        [0.8039216 ],\n",
              "        [0.8235294 ],\n",
              "        [0.8235294 ],\n",
              "        [0.81960785],\n",
              "        [0.7647059 ],\n",
              "        [0.8666667 ],\n",
              "        [0.3764706 ],\n",
              "        [0.        ],\n",
              "        [0.01176471],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.7921569 ],\n",
              "        [0.7882353 ],\n",
              "        [0.8039216 ],\n",
              "        [0.81960785],\n",
              "        [0.8117647 ],\n",
              "        [0.8039216 ],\n",
              "        [0.8352941 ],\n",
              "        [0.80784315],\n",
              "        [0.8235294 ],\n",
              "        [0.81960785],\n",
              "        [0.8235294 ],\n",
              "        [0.7607843 ],\n",
              "        [0.8509804 ],\n",
              "        [0.4117647 ],\n",
              "        [0.        ],\n",
              "        [0.00784314],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.8       ],\n",
              "        [0.8       ],\n",
              "        [0.8039216 ],\n",
              "        [0.8156863 ],\n",
              "        [0.8117647 ],\n",
              "        [0.8039216 ],\n",
              "        [0.84313726],\n",
              "        [0.8117647 ],\n",
              "        [0.8235294 ],\n",
              "        [0.8156863 ],\n",
              "        [0.827451  ],\n",
              "        [0.75686276],\n",
              "        [0.8352941 ],\n",
              "        [0.4509804 ],\n",
              "        [0.        ],\n",
              "        [0.00784314],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.8       ],\n",
              "        [0.8117647 ],\n",
              "        [0.8117647 ],\n",
              "        [0.8156863 ],\n",
              "        [0.80784315],\n",
              "        [0.80784315],\n",
              "        [0.84313726],\n",
              "        [0.8235294 ],\n",
              "        [0.8235294 ],\n",
              "        [0.8117647 ],\n",
              "        [0.83137256],\n",
              "        [0.7647059 ],\n",
              "        [0.8235294 ],\n",
              "        [0.4627451 ],\n",
              "        [0.        ],\n",
              "        [0.00784314],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.7764706 ],\n",
              "        [0.8156863 ],\n",
              "        [0.8156863 ],\n",
              "        [0.8156863 ],\n",
              "        [0.8       ],\n",
              "        [0.8117647 ],\n",
              "        [0.83137256],\n",
              "        [0.83137256],\n",
              "        [0.8235294 ],\n",
              "        [0.8117647 ],\n",
              "        [0.827451  ],\n",
              "        [0.76862746],\n",
              "        [0.8117647 ],\n",
              "        [0.4745098 ],\n",
              "        [0.        ],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.7764706 ],\n",
              "        [0.8235294 ],\n",
              "        [0.8117647 ],\n",
              "        [0.8156863 ],\n",
              "        [0.80784315],\n",
              "        [0.81960785],\n",
              "        [0.8352941 ],\n",
              "        [0.83137256],\n",
              "        [0.827451  ],\n",
              "        [0.8117647 ],\n",
              "        [0.8235294 ],\n",
              "        [0.77254903],\n",
              "        [0.8117647 ],\n",
              "        [0.4862745 ],\n",
              "        [0.        ],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.6745098 ],\n",
              "        [0.8235294 ],\n",
              "        [0.79607844],\n",
              "        [0.7882353 ],\n",
              "        [0.78039217],\n",
              "        [0.8       ],\n",
              "        [0.8117647 ],\n",
              "        [0.8039216 ],\n",
              "        [0.8       ],\n",
              "        [0.7882353 ],\n",
              "        [0.8039216 ],\n",
              "        [0.77254903],\n",
              "        [0.80784315],\n",
              "        [0.49803922],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.7372549 ],\n",
              "        [0.8666667 ],\n",
              "        [0.8392157 ],\n",
              "        [0.91764706],\n",
              "        [0.9254902 ],\n",
              "        [0.93333334],\n",
              "        [0.95686275],\n",
              "        [0.95686275],\n",
              "        [0.95686275],\n",
              "        [0.9411765 ],\n",
              "        [0.9529412 ],\n",
              "        [0.8392157 ],\n",
              "        [0.8784314 ],\n",
              "        [0.63529414],\n",
              "        [0.        ],\n",
              "        [0.00784314],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]],\n",
              "\n",
              "       [[0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.00392157],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.54509807],\n",
              "        [0.57254905],\n",
              "        [0.50980395],\n",
              "        [0.5294118 ],\n",
              "        [0.5294118 ],\n",
              "        [0.5372549 ],\n",
              "        [0.49019608],\n",
              "        [0.4862745 ],\n",
              "        [0.49019608],\n",
              "        [0.4745098 ],\n",
              "        [0.46666667],\n",
              "        [0.44705883],\n",
              "        [0.50980395],\n",
              "        [0.29803923],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ],\n",
              "        [0.        ]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFRRTJq8JjwQ",
        "colab_type": "text"
      },
      "source": [
        "### Import the necessary layers from keras to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWTZYnKSJjwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C18AoS7eJjwU",
        "colab_type": "text"
      },
      "source": [
        "### Build a model \n",
        "\n",
        "** with 2 Conv layers having `32 3x3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DORCLgSwJjwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "12cead3b-1d3d-46ee-ab83-ca3ea810e805"
      },
      "source": [
        "#Initialize the model\n",
        "model = Sequential()\n",
        "input_shape = (28, 28, 1)\n",
        "num_classes = 10\n",
        "batch_size = 128\n",
        "\n",
        "#Add a Convolutional Layer with 32 filters of size 3X3 and activation function as 'ReLU' \n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape,name='conv_1'))\n",
        "\n",
        "#Add a Convolutional Layer with 32 filters of size 3X3 and activation function as 'ReLU' \n",
        "model.add(Conv2D(32, (3, 3), activation='relu',name='conv_2'))\n",
        "\n",
        "#Flatten the layer\n",
        "model.add(Flatten())\n",
        "\n",
        "#Add Fully Connected Layer with 128 units and activation function as 'ReLU'\n",
        "model.add(Dense(128, activation='relu',name='dense_1'))\n",
        "\n",
        "\n",
        "#Add Fully Connected Layer with 10 units and activation function as 'softmax'\n",
        "model.add(Dense(num_classes, activation='softmax',name='dense_2'))\n",
        "\n",
        "# Store Training Results\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
        "callback_list = [early_stopping]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAMb6VP7QBD_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "13da14db-200c-4edb-b5c4-ca8baf1b5eaf"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_1 (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv_2 (Conv2D)              (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               2359424   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 2,370,282\n",
            "Trainable params: 2,370,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDdvCkuhQDgg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f3a5ea7e-a1d5-46da-b250-5c026a52dd0c"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "#To use adam optimizer for learning weights with learning rate = 0.001\n",
        "#optimizer = Adam(lr=0.001)\n",
        "optimizer = 'adam'\n",
        "#Set the loss function and optimizer for the model training\n",
        "model.compile(loss=categorical_crossentropy,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TKTkBbVR5LP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "7a84ad5c-b455-4b3b-b21b-95abc3546ee0"
      },
      "source": [
        "#Training on the dataset\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          verbose=1,\n",
        "          validation_split=0.1,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=callback_list)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.4203 - acc: 0.8507 - val_loss: 0.3390 - val_acc: 0.8745\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.2708 - acc: 0.9011 - val_loss: 0.2812 - val_acc: 0.9002\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.2182 - acc: 0.9193 - val_loss: 0.2758 - val_acc: 0.8995\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.1785 - acc: 0.9333 - val_loss: 0.2623 - val_acc: 0.9064\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.1430 - acc: 0.9473 - val_loss: 0.2688 - val_acc: 0.9102\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.1128 - acc: 0.9587 - val_loss: 0.2817 - val_acc: 0.9058\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0846 - acc: 0.9699 - val_loss: 0.2885 - val_acc: 0.9138\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0598 - acc: 0.9782 - val_loss: 0.3216 - val_acc: 0.9122\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0470 - acc: 0.9836 - val_loss: 0.3389 - val_acc: 0.9119\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0332 - acc: 0.9884 - val_loss: 0.3685 - val_acc: 0.9138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa7885f7e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju69vKdIJjwX",
        "colab_type": "text"
      },
      "source": [
        "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2hAP94vJjwY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "75331231-0d22-4b2d-f8df-45c96847a9ca"
      },
      "source": [
        "#Rebuild the model\n",
        "model1 = Sequential()\n",
        "input_shape = (28, 28, 1)\n",
        "num_classes = 10\n",
        "batch_size = 128\n",
        "\n",
        "#Add a Convolutional Layer with 32 filters of size 3X3 and activation function as 'ReLU' \n",
        "model1.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape,name='conv_1'))\n",
        "\n",
        "#Add a Convolutional Layer with 64 filters of size 3X3 and activation function as 'ReLU' \n",
        "model1.add(Conv2D(32, (3, 3), activation='relu',name='conv_2'))\n",
        "\n",
        "#Add a MaxPooling Layer of size 2X2 \n",
        "model1.add(MaxPooling2D(pool_size=(2, 2),name='max_1'))\n",
        "\n",
        "#Apply Dropout with 0.25 probability \n",
        "model1.add(Dropout(0.25,name='drop_1'))\n",
        "\n",
        "#Flatten the layer\n",
        "model1.add(Flatten())\n",
        "\n",
        "#Add Fully Connected Layer with 128 units and activation function as 'ReLU'\n",
        "model1.add(Dense(128, activation='relu',name='dense_1'))\n",
        "\n",
        "#Add Fully Connected Layer with 10 units and activation function as 'softmax'\n",
        "model1.add(Dense(num_classes, activation='softmax',name='dense_2'))\n",
        "\n",
        "model1.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_1 (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv_2 (Conv2D)              (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_1 (MaxPooling2D)         (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "drop_1 (Dropout)             (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               589952    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 600,810\n",
            "Trainable params: 600,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiVUDc24XKPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile new model\n",
        "model1.compile(loss=categorical_crossentropy,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oM1nkW1XSsr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "cd27c8b0-6a4a-4601-fc7a-23ffcce25d4d"
      },
      "source": [
        "#Training on the dataset on new model\n",
        "model1.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          verbose=1,\n",
        "          validation_split=0.1,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=callback_list)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.4542 - acc: 0.8392 - val_loss: 0.3351 - val_acc: 0.8817\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.2974 - acc: 0.8931 - val_loss: 0.2841 - val_acc: 0.8986\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 4s 70us/step - loss: 0.2493 - acc: 0.9077 - val_loss: 0.2577 - val_acc: 0.9042\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.2158 - acc: 0.9197 - val_loss: 0.2459 - val_acc: 0.9122\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.1916 - acc: 0.9293 - val_loss: 0.2302 - val_acc: 0.9163\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.1701 - acc: 0.9368 - val_loss: 0.2270 - val_acc: 0.9191\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 4s 72us/step - loss: 0.1508 - acc: 0.9440 - val_loss: 0.2231 - val_acc: 0.9199\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.1336 - acc: 0.9499 - val_loss: 0.2223 - val_acc: 0.9204\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.1189 - acc: 0.9549 - val_loss: 0.2293 - val_acc: 0.9217\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.1035 - acc: 0.9611 - val_loss: 0.2293 - val_acc: 0.9257\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa77a2e5cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttVWYMQIYJ5e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "326a8ca1-947b-4ea8-9fd7-fb6997feb392"
      },
      "source": [
        "loss_and_metrics = model1.evaluate(x_train, y_train)\n",
        "print(loss_and_metrics)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 3s 54us/step\n",
            "[0.06426038339411219, 0.9785833333333334]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGTA3bfEJjwa",
        "colab_type": "text"
      },
      "source": [
        "### Now, to the above model, lets add Data Augmentation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6gX8n5SJjwb",
        "colab_type": "text"
      },
      "source": [
        "### Import the ImageDataGenrator from keras and fit the training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbz4uHBuJjwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=50,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=True)  # randomly flip images\n",
        "\n",
        "# Prepare the generator\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl-8dOo7Jjwf",
        "colab_type": "text"
      },
      "source": [
        "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DpI1_McYJjwg",
        "colab_type": "code",
        "outputId": "ea5bc1e8-8fd2-4c4b-c00e-b6aedbb4e2a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "gen = datagen.flow(x_train[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABICAYAAABV5CYrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRL50E4lEokH8H8j0yOF0mUVdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmPl5yE8Jjwm",
        "colab_type": "text"
      },
      "source": [
        "### Run the above model using fit_generator()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44ZnDdJYJjwn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "c075b1ac-4e85-44fe-95b3-b2eda2d8f0db"
      },
      "source": [
        "model1.fit_generator(datagen.flow(x_train, y_train,batch_size=128),\n",
        "                    samples_per_epoch=x_train.shape[0],\n",
        "                    nb_epoch=20,\n",
        "                    validation_data=(x_test, y_test), callbacks=callback_list)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "  5/468 [..............................] - ETA: 15s - loss: 3.9124 - acc: 0.2141"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=468, epochs=20)`\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "468/468 [==============================] - 15s 32ms/step - loss: 1.3933 - acc: 0.4859 - val_loss: 0.7648 - val_acc: 0.7108\n",
            "Epoch 2/20\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 1.0218 - acc: 0.6264 - val_loss: 0.7614 - val_acc: 0.7185\n",
            "Epoch 3/20\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.9247 - acc: 0.6641 - val_loss: 0.7989 - val_acc: 0.7136\n",
            "Epoch 4/20\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.8539 - acc: 0.6909 - val_loss: 0.6298 - val_acc: 0.7775\n",
            "Epoch 5/20\n",
            "468/468 [==============================] - 15s 31ms/step - loss: 0.8043 - acc: 0.7056 - val_loss: 0.6277 - val_acc: 0.7687\n",
            "Epoch 6/20\n",
            "468/468 [==============================] - 15s 31ms/step - loss: 0.7770 - acc: 0.7156 - val_loss: 0.6768 - val_acc: 0.7600\n",
            "Epoch 7/20\n",
            "468/468 [==============================] - 15s 33ms/step - loss: 0.7531 - acc: 0.7271 - val_loss: 0.5852 - val_acc: 0.7918\n",
            "Epoch 8/20\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.7297 - acc: 0.7342 - val_loss: 0.6107 - val_acc: 0.7893\n",
            "Epoch 9/20\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.7068 - acc: 0.7429 - val_loss: 0.6564 - val_acc: 0.7702\n",
            "Epoch 10/20\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.6957 - acc: 0.7455 - val_loss: 0.5866 - val_acc: 0.7969\n",
            "Epoch 11/20\n",
            "468/468 [==============================] - 15s 33ms/step - loss: 0.6851 - acc: 0.7502 - val_loss: 0.6018 - val_acc: 0.7901\n",
            "Epoch 12/20\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.6707 - acc: 0.7551 - val_loss: 0.5674 - val_acc: 0.8048\n",
            "Epoch 13/20\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.6578 - acc: 0.7595 - val_loss: 0.6582 - val_acc: 0.7772\n",
            "Epoch 14/20\n",
            "468/468 [==============================] - 15s 31ms/step - loss: 0.6463 - acc: 0.7640 - val_loss: 0.5632 - val_acc: 0.8043\n",
            "Epoch 15/20\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.6383 - acc: 0.7654 - val_loss: 0.5580 - val_acc: 0.8058\n",
            "Epoch 16/20\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.6273 - acc: 0.7711 - val_loss: 0.5630 - val_acc: 0.8023\n",
            "Epoch 17/20\n",
            "468/468 [==============================] - 15s 31ms/step - loss: 0.6191 - acc: 0.7751 - val_loss: 0.5261 - val_acc: 0.8085\n",
            "Epoch 18/20\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.6101 - acc: 0.7778 - val_loss: 0.5182 - val_acc: 0.8165\n",
            "Epoch 19/20\n",
            "468/468 [==============================] - 15s 33ms/step - loss: 0.6078 - acc: 0.7765 - val_loss: 0.5235 - val_acc: 0.8170\n",
            "Epoch 20/20\n",
            "468/468 [==============================] - 15s 32ms/step - loss: 0.6002 - acc: 0.7814 - val_loss: 0.5116 - val_acc: 0.8163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa77a2d1278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwQQW5iOJjwq",
        "colab_type": "text"
      },
      "source": [
        "###  Report the final train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1SrtBEPJjwq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7a33df82-c04e-445c-cdda-55643bc5a71b"
      },
      "source": [
        "loss_and_metrics = model1.evaluate(x_train, y_train)\n",
        "print(loss_and_metrics)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 3s 56us/step\n",
            "[0.48934387852748235, 0.82175]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KXqmUDW2rM1",
        "colab_type": "text"
      },
      "source": [
        "## **DATA AUGMENTATION ON CIFAR10 DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mja6OgQ3L18",
        "colab_type": "text"
      },
      "source": [
        "One of the best ways to improve the performance of a Deep Learning model is to add more data to the training set. Aside from gathering more instances from the wild that are representative of the distinction task, we want to develop a set of methods that enhance the data we already have. There are many ways to augment existing datasets and produce more robust models. In the image domain, these are done to utilize the full power of the convolutional neural network, which is able to capture translational invariance. This translational invariance is what makes image recognition such a difficult task in the first place. You want the dataset to be representative of the many different positions, angles, lightings, and miscellaneous distortions that are of interest to the vision task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HzVTPUM3WZJ",
        "colab_type": "text"
      },
      "source": [
        "### **Import necessary libraries for data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPM558TX4KMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6hicLwP4SqY",
        "colab_type": "text"
      },
      "source": [
        "### **Load CIFAR10 dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ1WzrXd4WNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9Pht1ggHuiT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b185c6a8-4f1e-4df0-f319-31ca69f762b5"
      },
      "source": [
        "print(x_train, y_train)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[ 59  62  63]\n",
            "   [ 43  46  45]\n",
            "   [ 50  48  43]\n",
            "   ...\n",
            "   [158 132 108]\n",
            "   [152 125 102]\n",
            "   [148 124 103]]\n",
            "\n",
            "  [[ 16  20  20]\n",
            "   [  0   0   0]\n",
            "   [ 18   8   0]\n",
            "   ...\n",
            "   [123  88  55]\n",
            "   [119  83  50]\n",
            "   [122  87  57]]\n",
            "\n",
            "  [[ 25  24  21]\n",
            "   [ 16   7   0]\n",
            "   [ 49  27   8]\n",
            "   ...\n",
            "   [118  84  50]\n",
            "   [120  84  50]\n",
            "   [109  73  42]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[208 170  96]\n",
            "   [201 153  34]\n",
            "   [198 161  26]\n",
            "   ...\n",
            "   [160 133  70]\n",
            "   [ 56  31   7]\n",
            "   [ 53  34  20]]\n",
            "\n",
            "  [[180 139  96]\n",
            "   [173 123  42]\n",
            "   [186 144  30]\n",
            "   ...\n",
            "   [184 148  94]\n",
            "   [ 97  62  34]\n",
            "   [ 83  53  34]]\n",
            "\n",
            "  [[177 144 116]\n",
            "   [168 129  94]\n",
            "   [179 142  87]\n",
            "   ...\n",
            "   [216 184 140]\n",
            "   [151 118  84]\n",
            "   [123  92  72]]]\n",
            "\n",
            "\n",
            " [[[154 177 187]\n",
            "   [126 137 136]\n",
            "   [105 104  95]\n",
            "   ...\n",
            "   [ 91  95  71]\n",
            "   [ 87  90  71]\n",
            "   [ 79  81  70]]\n",
            "\n",
            "  [[140 160 169]\n",
            "   [145 153 154]\n",
            "   [125 125 118]\n",
            "   ...\n",
            "   [ 96  99  78]\n",
            "   [ 77  80  62]\n",
            "   [ 71  73  61]]\n",
            "\n",
            "  [[140 155 164]\n",
            "   [139 146 149]\n",
            "   [115 115 112]\n",
            "   ...\n",
            "   [ 79  82  64]\n",
            "   [ 68  70  55]\n",
            "   [ 67  69  55]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[175 167 166]\n",
            "   [156 154 160]\n",
            "   [154 160 170]\n",
            "   ...\n",
            "   [ 42  34  36]\n",
            "   [ 61  53  57]\n",
            "   [ 93  83  91]]\n",
            "\n",
            "  [[165 154 128]\n",
            "   [156 152 130]\n",
            "   [159 161 142]\n",
            "   ...\n",
            "   [103  93  96]\n",
            "   [123 114 120]\n",
            "   [131 121 131]]\n",
            "\n",
            "  [[163 148 120]\n",
            "   [158 148 122]\n",
            "   [163 156 133]\n",
            "   ...\n",
            "   [143 133 139]\n",
            "   [143 134 142]\n",
            "   [143 133 144]]]\n",
            "\n",
            "\n",
            " [[[255 255 255]\n",
            "   [253 253 253]\n",
            "   [253 253 253]\n",
            "   ...\n",
            "   [253 253 253]\n",
            "   [253 253 253]\n",
            "   [253 253 253]]\n",
            "\n",
            "  [[255 255 255]\n",
            "   [255 255 255]\n",
            "   [255 255 255]\n",
            "   ...\n",
            "   [255 255 255]\n",
            "   [255 255 255]\n",
            "   [255 255 255]]\n",
            "\n",
            "  [[255 255 255]\n",
            "   [254 254 254]\n",
            "   [254 254 254]\n",
            "   ...\n",
            "   [254 254 254]\n",
            "   [254 254 254]\n",
            "   [254 254 254]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[113 120 112]\n",
            "   [111 118 111]\n",
            "   [105 112 106]\n",
            "   ...\n",
            "   [ 72  81  80]\n",
            "   [ 72  80  79]\n",
            "   [ 72  80  79]]\n",
            "\n",
            "  [[111 118 110]\n",
            "   [104 111 104]\n",
            "   [ 99 106  98]\n",
            "   ...\n",
            "   [ 68  75  73]\n",
            "   [ 70  76  75]\n",
            "   [ 78  84  82]]\n",
            "\n",
            "  [[106 113 105]\n",
            "   [ 99 106  98]\n",
            "   [ 95 102  94]\n",
            "   ...\n",
            "   [ 78  85  83]\n",
            "   [ 79  85  83]\n",
            "   [ 80  86  84]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 35 178 235]\n",
            "   [ 40 176 239]\n",
            "   [ 42 176 241]\n",
            "   ...\n",
            "   [ 99 177 219]\n",
            "   [ 79 147 197]\n",
            "   [ 89 148 189]]\n",
            "\n",
            "  [[ 57 182 234]\n",
            "   [ 44 184 250]\n",
            "   [ 50 183 240]\n",
            "   ...\n",
            "   [156 182 200]\n",
            "   [141 177 206]\n",
            "   [116 149 175]]\n",
            "\n",
            "  [[ 98 197 237]\n",
            "   [ 64 189 252]\n",
            "   [ 69 192 245]\n",
            "   ...\n",
            "   [188 195 206]\n",
            "   [119 135 147]\n",
            "   [ 61  79  90]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 73  79  77]\n",
            "   [ 53  63  68]\n",
            "   [ 54  68  80]\n",
            "   ...\n",
            "   [ 17  40  64]\n",
            "   [ 21  36  51]\n",
            "   [ 33  48  49]]\n",
            "\n",
            "  [[ 61  68  75]\n",
            "   [ 55  70  86]\n",
            "   [ 57  79 103]\n",
            "   ...\n",
            "   [ 24  48  72]\n",
            "   [ 17  35  53]\n",
            "   [  7  23  32]]\n",
            "\n",
            "  [[ 44  56  73]\n",
            "   [ 46  66  88]\n",
            "   [ 49  77 105]\n",
            "   ...\n",
            "   [ 27  52  77]\n",
            "   [ 21  43  66]\n",
            "   [ 12  31  50]]]\n",
            "\n",
            "\n",
            " [[[189 211 240]\n",
            "   [186 208 236]\n",
            "   [185 207 235]\n",
            "   ...\n",
            "   [175 195 224]\n",
            "   [172 194 222]\n",
            "   [169 194 220]]\n",
            "\n",
            "  [[194 210 239]\n",
            "   [191 207 236]\n",
            "   [190 206 235]\n",
            "   ...\n",
            "   [173 192 220]\n",
            "   [171 191 218]\n",
            "   [167 190 216]]\n",
            "\n",
            "  [[208 219 244]\n",
            "   [205 216 240]\n",
            "   [204 215 239]\n",
            "   ...\n",
            "   [175 191 217]\n",
            "   [172 190 216]\n",
            "   [169 191 215]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[207 199 181]\n",
            "   [203 195 175]\n",
            "   [203 196 173]\n",
            "   ...\n",
            "   [135 132 127]\n",
            "   [162 158 150]\n",
            "   [168 163 151]]\n",
            "\n",
            "  [[198 190 170]\n",
            "   [189 181 159]\n",
            "   [180 172 147]\n",
            "   ...\n",
            "   [178 171 160]\n",
            "   [175 169 156]\n",
            "   [175 169 154]]\n",
            "\n",
            "  [[198 189 173]\n",
            "   [189 181 162]\n",
            "   [178 170 149]\n",
            "   ...\n",
            "   [195 184 169]\n",
            "   [196 189 171]\n",
            "   [195 190 171]]]\n",
            "\n",
            "\n",
            " [[[229 229 239]\n",
            "   [236 237 247]\n",
            "   [234 236 247]\n",
            "   ...\n",
            "   [217 219 233]\n",
            "   [221 223 234]\n",
            "   [222 223 233]]\n",
            "\n",
            "  [[222 221 229]\n",
            "   [239 239 249]\n",
            "   [233 234 246]\n",
            "   ...\n",
            "   [223 223 236]\n",
            "   [227 228 238]\n",
            "   [210 211 220]]\n",
            "\n",
            "  [[213 206 211]\n",
            "   [234 232 239]\n",
            "   [231 233 244]\n",
            "   ...\n",
            "   [220 220 232]\n",
            "   [220 219 232]\n",
            "   [202 203 215]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[150 143 135]\n",
            "   [140 135 127]\n",
            "   [132 127 120]\n",
            "   ...\n",
            "   [224 222 218]\n",
            "   [230 228 225]\n",
            "   [241 241 238]]\n",
            "\n",
            "  [[137 132 126]\n",
            "   [130 127 120]\n",
            "   [125 121 115]\n",
            "   ...\n",
            "   [181 180 178]\n",
            "   [202 201 198]\n",
            "   [212 211 207]]\n",
            "\n",
            "  [[122 119 114]\n",
            "   [118 116 110]\n",
            "   [120 116 111]\n",
            "   ...\n",
            "   [179 177 173]\n",
            "   [164 164 162]\n",
            "   [163 163 161]]]] [[6]\n",
            " [9]\n",
            " [9]\n",
            " ...\n",
            " [9]\n",
            " [1]\n",
            " [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n28ccU6Hp6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5844522-b527-4a92-cd01-3f97cf41dabb"
      },
      "source": [
        "print(x_test, y_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[158 112  49]\n",
            "   [159 111  47]\n",
            "   [165 116  51]\n",
            "   ...\n",
            "   [137  95  36]\n",
            "   [126  91  36]\n",
            "   [116  85  33]]\n",
            "\n",
            "  [[152 112  51]\n",
            "   [151 110  40]\n",
            "   [159 114  45]\n",
            "   ...\n",
            "   [136  95  31]\n",
            "   [125  91  32]\n",
            "   [119  88  34]]\n",
            "\n",
            "  [[151 110  47]\n",
            "   [151 109  33]\n",
            "   [158 111  36]\n",
            "   ...\n",
            "   [139  98  34]\n",
            "   [130  95  34]\n",
            "   [120  89  33]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 68 124 177]\n",
            "   [ 42 100 148]\n",
            "   [ 31  88 137]\n",
            "   ...\n",
            "   [ 38  97 146]\n",
            "   [ 13  64 108]\n",
            "   [ 40  85 127]]\n",
            "\n",
            "  [[ 61 116 168]\n",
            "   [ 49 102 148]\n",
            "   [ 35  85 132]\n",
            "   ...\n",
            "   [ 26  82 130]\n",
            "   [ 29  82 126]\n",
            "   [ 20  64 107]]\n",
            "\n",
            "  [[ 54 107 160]\n",
            "   [ 56 105 149]\n",
            "   [ 45  89 132]\n",
            "   ...\n",
            "   [ 24  77 124]\n",
            "   [ 34  84 129]\n",
            "   [ 21  67 110]]]\n",
            "\n",
            "\n",
            " [[[235 235 235]\n",
            "   [231 231 231]\n",
            "   [232 232 232]\n",
            "   ...\n",
            "   [233 233 233]\n",
            "   [233 233 233]\n",
            "   [232 232 232]]\n",
            "\n",
            "  [[238 238 238]\n",
            "   [235 235 235]\n",
            "   [235 235 235]\n",
            "   ...\n",
            "   [236 236 236]\n",
            "   [236 236 236]\n",
            "   [235 235 235]]\n",
            "\n",
            "  [[237 237 237]\n",
            "   [234 234 234]\n",
            "   [234 234 234]\n",
            "   ...\n",
            "   [235 235 235]\n",
            "   [235 235 235]\n",
            "   [234 234 234]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 87  99  89]\n",
            "   [ 43  51  37]\n",
            "   [ 19  23  11]\n",
            "   ...\n",
            "   [169 184 179]\n",
            "   [182 197 193]\n",
            "   [188 202 201]]\n",
            "\n",
            "  [[ 82  96  82]\n",
            "   [ 46  57  36]\n",
            "   [ 36  44  22]\n",
            "   ...\n",
            "   [174 189 183]\n",
            "   [185 200 196]\n",
            "   [187 202 200]]\n",
            "\n",
            "  [[ 85 101  83]\n",
            "   [ 62  75  48]\n",
            "   [ 58  67  38]\n",
            "   ...\n",
            "   [168 183 178]\n",
            "   [180 195 191]\n",
            "   [186 200 199]]]\n",
            "\n",
            "\n",
            " [[[158 190 222]\n",
            "   [158 187 218]\n",
            "   [139 166 194]\n",
            "   ...\n",
            "   [228 231 234]\n",
            "   [237 239 243]\n",
            "   [238 241 246]]\n",
            "\n",
            "  [[170 200 229]\n",
            "   [172 199 226]\n",
            "   [151 176 201]\n",
            "   ...\n",
            "   [232 232 236]\n",
            "   [246 246 250]\n",
            "   [246 247 251]]\n",
            "\n",
            "  [[174 201 225]\n",
            "   [176 200 222]\n",
            "   [157 179 199]\n",
            "   ...\n",
            "   [230 229 232]\n",
            "   [250 249 251]\n",
            "   [245 244 247]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 31  40  45]\n",
            "   [ 30  39  44]\n",
            "   [ 26  35  40]\n",
            "   ...\n",
            "   [ 37  40  46]\n",
            "   [  9  13  14]\n",
            "   [  4   7   5]]\n",
            "\n",
            "  [[ 23  34  39]\n",
            "   [ 27  38  43]\n",
            "   [ 25  36  41]\n",
            "   ...\n",
            "   [ 19  20  24]\n",
            "   [  4   6   3]\n",
            "   [  5   7   3]]\n",
            "\n",
            "  [[ 28  41  47]\n",
            "   [ 30  43  50]\n",
            "   [ 32  45  52]\n",
            "   ...\n",
            "   [  5   6   8]\n",
            "   [  4   5   3]\n",
            "   [  7   8   7]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 20  15  12]\n",
            "   [ 19  14  11]\n",
            "   [ 15  14  11]\n",
            "   ...\n",
            "   [ 10   9   7]\n",
            "   [ 12  11   9]\n",
            "   [ 13  12  10]]\n",
            "\n",
            "  [[ 21  16  13]\n",
            "   [ 20  16  13]\n",
            "   [ 18  17  12]\n",
            "   ...\n",
            "   [ 10   9   7]\n",
            "   [ 10   9   7]\n",
            "   [ 12  11   9]]\n",
            "\n",
            "  [[ 21  16  13]\n",
            "   [ 21  17  12]\n",
            "   [ 20  18  11]\n",
            "   ...\n",
            "   [ 12  11   9]\n",
            "   [ 12  11   9]\n",
            "   [ 13  12  10]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 33  25  13]\n",
            "   [ 34  26  15]\n",
            "   [ 34  26  15]\n",
            "   ...\n",
            "   [ 28  25  52]\n",
            "   [ 29  25  58]\n",
            "   [ 23  20  42]]\n",
            "\n",
            "  [[ 33  25  14]\n",
            "   [ 34  26  15]\n",
            "   [ 34  26  15]\n",
            "   ...\n",
            "   [ 27  24  52]\n",
            "   [ 27  24  56]\n",
            "   [ 25  22  47]]\n",
            "\n",
            "  [[ 31  23  12]\n",
            "   [ 32  24  13]\n",
            "   [ 33  25  14]\n",
            "   ...\n",
            "   [ 24  23  50]\n",
            "   [ 26  23  53]\n",
            "   [ 25  20  47]]]\n",
            "\n",
            "\n",
            " [[[ 25  40  12]\n",
            "   [ 15  36   3]\n",
            "   [ 23  41  18]\n",
            "   ...\n",
            "   [ 61  82  78]\n",
            "   [ 92 113 112]\n",
            "   [ 75  89  92]]\n",
            "\n",
            "  [[ 12  25   6]\n",
            "   [ 20  37   7]\n",
            "   [ 24  36  15]\n",
            "   ...\n",
            "   [115 134 138]\n",
            "   [149 168 177]\n",
            "   [104 117 131]]\n",
            "\n",
            "  [[ 12  25  11]\n",
            "   [ 15  29   6]\n",
            "   [ 34  40  24]\n",
            "   ...\n",
            "   [154 172 182]\n",
            "   [157 175 192]\n",
            "   [116 129 151]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[100 129  81]\n",
            "   [103 132  84]\n",
            "   [104 134  86]\n",
            "   ...\n",
            "   [ 97 128  84]\n",
            "   [ 98 126  84]\n",
            "   [ 91 121  79]]\n",
            "\n",
            "  [[103 132  83]\n",
            "   [104 131  83]\n",
            "   [107 135  87]\n",
            "   ...\n",
            "   [101 132  87]\n",
            "   [ 99 127  84]\n",
            "   [ 92 121  79]]\n",
            "\n",
            "  [[ 95 126  78]\n",
            "   [ 95 123  76]\n",
            "   [101 128  81]\n",
            "   ...\n",
            "   [ 93 124  80]\n",
            "   [ 95 123  81]\n",
            "   [ 92 120  80]]]\n",
            "\n",
            "\n",
            " [[[ 73  78  75]\n",
            "   [ 98 103 113]\n",
            "   [ 99 106 114]\n",
            "   ...\n",
            "   [135 150 152]\n",
            "   [135 149 154]\n",
            "   [203 215 223]]\n",
            "\n",
            "  [[ 69  73  70]\n",
            "   [ 84  89  97]\n",
            "   [ 68  75  81]\n",
            "   ...\n",
            "   [ 85  95  89]\n",
            "   [ 71  82  80]\n",
            "   [120 133 135]]\n",
            "\n",
            "  [[ 69  73  70]\n",
            "   [ 90  95 100]\n",
            "   [ 62  71  74]\n",
            "   ...\n",
            "   [ 74  81  70]\n",
            "   [ 53  62  54]\n",
            "   [ 62  74  69]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[123 128  96]\n",
            "   [132 132 102]\n",
            "   [129 128 100]\n",
            "   ...\n",
            "   [108 107  88]\n",
            "   [ 62  60  55]\n",
            "   [ 27  27  28]]\n",
            "\n",
            "  [[115 121  91]\n",
            "   [123 124  95]\n",
            "   [129 126  99]\n",
            "   ...\n",
            "   [115 116  94]\n",
            "   [ 66  65  59]\n",
            "   [ 27  27  27]]\n",
            "\n",
            "  [[116 120  90]\n",
            "   [121 122  94]\n",
            "   [129 128 101]\n",
            "   ...\n",
            "   [116 115  94]\n",
            "   [ 68  65  58]\n",
            "   [ 27  26  26]]]] [[3]\n",
            " [8]\n",
            " [8]\n",
            " ...\n",
            " [5]\n",
            " [1]\n",
            " [7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN3vYYhK4W0u",
        "colab_type": "text"
      },
      "source": [
        "### **Create a data_gen funtion to genererator with image rotation,shifting image horizontally and vertically with random flip horizontally.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJbekTKi4cmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=45,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-SLtUhC4dK2",
        "colab_type": "text"
      },
      "source": [
        "### **Prepare/fit the generator.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSw8Bv2_4hb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare the generator\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYyF-P8O4jQ8",
        "colab_type": "text"
      },
      "source": [
        "### **Generate 5 images for 1 of the image of CIFAR10 train dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXug4z234mwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "fdfb5ad0-c286-4ad9-e725-cf516f03fe2f"
      },
      "source": [
        "gen = datagen.flow(x_train[:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze().astype('uint8'), cmap='gray')\n",
        "    plt.plot()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABICAYAAABV5CYrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwwe09yznVsN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "7eda4088-0c53-4403-d8ab-73ea64f1b83c"
      },
      "source": [
        "plt.imshow(x_train[0].squeeze().astype('uint8'), cmap='gray')\n",
        "plt.plot()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}